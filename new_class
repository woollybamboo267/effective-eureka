
torch.set_num_threads(4)
class MultiHeadActionMaskRLModule(TorchRLModule, ValueFunctionAPI):
    """Multi-head categorical policy with counterfactual baselines and gradient stability"""
        
    def __init__(self, config=None, observation_space=None, action_space=None, inference_only=False, learner_only=False, model_config=None, catalog_class=None, **kwargs):
        """Initialize the RLModule with new API - handles catalog_class and other RLLib parameters"""
        # Handle both old and new API for compatibility
      
        self._cache_lock = threading.Lock()
        self.num_cpus = mp.cpu_count()
        self.parallel_workers = max(2, int(self.num_cpus * 0.75))
        self.cpu_worker_pool = ThreadPoolExecutor(max_workers=self.parallel_workers)
        
  
        if config is not None:
            # Old API - extract parameters from config
            if hasattr(config, 'observation_space'):
                observation_space = config.observation_space
                action_space = config.action_space
                inference_only = getattr(config, 'inference_only', False)
                learner_only = getattr(config, 'learner_only', False)
                model_config = getattr(config, 'model_config', {})
            else:
                # Config is a dict
                observation_space = config.get('observation_space', observation_space)
                action_space = config.get('action_space', action_space)
                inference_only = config.get('inference_only', inference_only)
                learner_only = config.get('learner_only', learner_only)
                model_config = config.get('model_config', model_config or {})
        
        # ULTRA-FAST: Store catalog_class but don't use it (RLLib compatibility)
        self.catalog_class = catalog_class
        
        # Use new API with all supported parameters
        super().__init__(
            observation_space=observation_space,
            action_space=action_space,
            inference_only=inference_only,
            learner_only=learner_only,
            model_config=model_config or {},
            **kwargs  # Pass through any additional RLLib parameters
        )
        
        # Store config for backward compatibility
        if config is not None:
            self.config = config
        else:
            # Create config-like object for backward compatibility
            class ConfigLike:
                def __init__(self, obs_space, act_space, inf_only, learn_only, mod_config):
                    self.observation_space = obs_space
                    self.action_space = act_space
                    self.inference_only = inf_only
                    self.learner_only = learn_only
                    self.model_config = mod_config or {}
            
            self.config = ConfigLike(observation_space, action_space, inference_only, learner_only, model_config or {})
        
        # ULTRA-FAST: Pre-allocated tensor pools and caches
        self._tensor_pool = {}
        self._attention_cache = {}
        self._encoder_cache = {}
        self._mask_slice_cache = None
        self._component_batch_cache = {}
        self._q_input_cache = {}
        
        # Lightweight logging counters for learning vs sampling activity
        self._learning_count = 0
        self._sampling_count = 0
        self._log_every_n = 50  # Log every 50 calls to avoid spam
        
        # ULTRA-FAST: Initialize SAC components immediately to reduce setup time
        self._setup_sac_components()
        
        # STATE-OF-THE-ART: Initialize LSTM for temporal learning AFTER SAC components
        self._setup_temporal_lstm()
        
        # CRITICAL: Initialize device tracking and move to appropriate device
        self._init_device_management()
        
    def _init_device_management(self):
        """Initialize proper device management for the module"""
        # FIXED: Detect actual GPU allocation, not just CUDA availability
        worker_id = get_ray_worker_id()
        log_extra = {'worker_id': worker_id}
        
        # Check if we actually have GPU resources allocated (not just CUDA available)
        has_gpu_allocation = False
        
        if torch.cuda.is_available():
            try:
                # Check if we can actually use CUDA (GPU allocated)
                test_tensor = torch.zeros(1, device='cuda')
                has_gpu_allocation = True
                del test_tensor
                worker_logger.warning(f"[DEVICE] GPU allocation confirmed - can use CUDA", extra=log_extra)
            except (RuntimeError, Exception) as e:
                # No GPU allocated to this worker
                has_gpu_allocation = False
                worker_logger.warning(f"[DEVICE] No GPU allocation - CUDA available but not allocated: {e}", extra=log_extra)
        else:
            worker_logger.warning(f"[DEVICE] CUDA not available", extra=log_extra)
        
        # Set device based on actual GPU allocation
        if has_gpu_allocation:
            try:
                # Check if we're in a learner context (training) or env runner context (sampling)
                import ray
                if ray.is_initialized():
                    # IMPROVED: Better context detection
                    try:
                        # Check for learner group context
                        import os
                        cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', '')
                        ray_task_name = os.environ.get('RAY_TASK_NAME', '')
                        
                        # Detect training context more reliably
                        is_learner = (
                            'learner' in ray_task_name.lower() or
                            'LearnerGroup' in str(type(self)) or
                            hasattr(self, '_is_learner_context')
                        )
                        
                        if is_learner:
                            self.device = torch.device("cuda")
                            self.use_gpu = True
                            worker_logger.warning(f"[DEVICE] Training/Learner context detected - using GPU (CUDA_VISIBLE: {cuda_visible}, TASK: {ray_task_name})", extra=log_extra)
                        else:
                            # Even if GPU available, env runners should use CPU for compatibility
                            self.device = torch.device("cpu")
                            self.use_gpu = False
                            worker_logger.warning(f"[DEVICE] Sampling/EnvRunner context detected - using CPU (GPU available but not used)", extra=log_extra)
                    except Exception as inner_e:
                        # Default to GPU if we have allocation and unclear context
                        self.device = torch.device("cuda")
                        self.use_gpu = True
                        worker_logger.warning(f"[DEVICE] GPU allocated but unclear context - using GPU: {inner_e}", extra=log_extra)
                else:
                    # Not in Ray context but have GPU allocation
                    self.device = torch.device("cuda")
                    self.use_gpu = True
                    worker_logger.warning(f"[DEVICE] Non-Ray context with GPU allocation - using GPU", extra=log_extra)
            except Exception as e:
                # Fallback to CPU if any error
                self.device = torch.device("cpu")
                self.use_gpu = False
                worker_logger.warning(f"[DEVICE] Error in GPU context detection, using CPU: {e}", extra=log_extra)
        else:
            # No GPU allocation or CUDA not available
            self.device = torch.device("cpu")
            self.use_gpu = False
            worker_logger.warning(f"[DEVICE] No GPU allocation - using CPU", extra=log_extra)
        
        # Move module to device
        self.to(self.device)
        worker_logger.warning(f"[DEVICE] Module moved to {self.device}", extra=log_extra)
        
        # Verify device placement
        try:
            sample_param = next(self.parameters())
            worker_logger.warning(f"[DEVICE] Verified model parameters on: {sample_param.device}", extra=log_extra)
        except StopIteration:
            worker_logger.warning(f"[DEVICE] No parameters found to verify device", extra={'worker_id': get_ray_worker_id()})
    
    def _ensure_tensor_device(self, tensor, name="tensor"):
        """Ensure tensor is on the correct device with robust fallback - CONSERVATIVE APPROACH"""
        if isinstance(tensor, torch.Tensor):
            # Determine target device dynamically and conservatively
            try:
                # First priority: use self.device if it's set and valid
                if hasattr(self, 'device') and self.device is not None:
                    target_device = self.device
                else:
                    # Fallback: Get device from model parameters
                    try:
                        target_device = next(self.parameters()).device
                        worker_logger.warning(f"[DEVICE] self.device not set, using model device: {target_device}", extra={'worker_id': get_ray_worker_id()})
                    except StopIteration:
                        # Ultimate fallback: keep tensor on its current device (CONSERVATIVE)
                        target_device = tensor.device
                        worker_logger.warning(f"[DEVICE] No model parameters found, keeping tensor on: {target_device}", extra={'worker_id': get_ray_worker_id()})
                
                # CONSERVATIVE: Only move to CUDA if we can actually test it works
                if target_device.type == 'cuda' and tensor.device.type != 'cuda':
                    try:
                        # Test if we can actually use CUDA
                        test_tensor = torch.zeros(1, device=target_device)
                        del test_tensor
                        # OK to move to CUDA
                        worker_logger.warning(f"[DEVICE] Moving {name} from {tensor.device} to {target_device}", extra={'worker_id': get_ray_worker_id()})
                        return tensor.to(target_device, non_blocking=True)
                    except (RuntimeError, Exception) as e:
                        # Can't use CUDA, keep on CPU
                        worker_logger.warning(f"[DEVICE] Cannot move {name} to {target_device}, keeping on CPU: {e}", extra={'worker_id': get_ray_worker_id()})
                        return tensor
                elif tensor.device != target_device:
                    # Safe device move (non-CUDA or CUDA->CUDA)
                    worker_logger.warning(f"[DEVICE] Moving {name} from {tensor.device} to {target_device}", extra={'worker_id': get_ray_worker_id()})
                    return tensor.to(target_device, non_blocking=True)
                
            except Exception as e:
                worker_logger.warning(f"[DEVICE] Error in device management for {name}, keeping original: {e}", extra={'worker_id': get_ray_worker_id()})
                return tensor
                
        return tensor
    
    def _compute_heads_with_attention_masking(self, trunk_features, attention_mask):
        """Compute action logits with attention masking - ensures no padding influence"""
        # Apply attention masking to trunk features before head computation
        if attention_mask is not None:
            # Apply mask: padded positions get zero features
            masked_features = trunk_features * attention_mask.unsqueeze(-1)
        else:
            masked_features = trunk_features
        
        # Compute all heads on masked features (no padding effects)
        head_outputs = self._compute_all_heads_batch(masked_features)
        
        return head_outputs

    def _compute_values_with_attention_masking(self, policy_features, attention_mask):
        """Compute value function with attention masking - ensures no padding influence"""
        if attention_mask is not None:
            # Apply mask: padded positions get zero features  
            masked_features = policy_features * attention_mask.unsqueeze(-1)
        else:
            masked_features = policy_features
            
        # Compute values on masked features (no padding effects)
        values = self.value_head(masked_features)
        
        return values
    
    def _compute_action_logits_with_masking(self, encoded, action_mask, attention_mask=None):
        """Compute action logits with optional attention masking.
        
        Args:
            encoded: Encoded observations
            action_mask: Boolean mask for valid actions
            attention_mask: Optional attention mask for padding isolation
        
        Returns:
            Masked action logits where padding doesn't affect valid computations
        """
        # Compute base action logits
        action_logits = self.action_head(encoded)
        
        # Apply action masking for invalid actions
        if action_mask is not None:
            if action_mask.dtype != torch.bool:
                action_mask = action_mask.bool()
            # Set invalid actions to very negative value to prevent selection
            action_logits = torch.where(action_mask, action_logits, torch.tensor(-1e8, device=action_logits.device))
        
        # Apply attention masking if provided to isolate padding
        if attention_mask is not None:
            action_logits = self._apply_attention_mask_to_logits(action_logits, attention_mask)
        
        return action_logits
    
    def _ensure_batch_device(self, batch):
        """Ensure all tensors in batch are on the correct device"""
        if isinstance(batch, dict):
            return {k: self._ensure_tensor_device(v, f"batch[{k}]") for k, v in batch.items()}
        elif isinstance(batch, torch.Tensor):
            return self._ensure_tensor_device(batch, "batch")
        elif isinstance(batch, (list, tuple)):
            return [self._ensure_tensor_device(item, f"batch[{i}]") for i, item in enumerate(batch)]
        return batch
        
    def clear_episode_caches(self):
        """Clear all caches at episode boundaries with debugging"""
        
        # DEBUG: Log cache states before clearing
        worker_logger.warning(
            f"Pre-clear cache sizes: encoder={len(self._encoder_cache)}, "
            f"q_input={len(self._q_input_cache)}, "
            f"attention={len(self._attention_cache)}, "
            f"tensor_pool={len(self._tensor_pool)}",
            extra={'worker_id': get_ray_worker_id()}
        )
        
        # FIX: Nuclear clear - don't keep any cached tensors
        self._encoder_cache.clear()
        self._component_batch_cache.clear() 
        self._q_input_cache.clear()
        self._attention_cache.clear()
        self._tensor_pool.clear()  # Clear everything, don't keep any
        
        # FIX: Clear mask cache that might accumulate
        self._mask_slice_cache = None
        
        # FIX: Force clear dynamic fusion networks completely
        if hasattr(self, 'qf_encoder') and hasattr(self.qf_encoder, 'fusion_networks'):
            self.qf_encoder.fusion_networks.clear()
            worker_logger.warning("Cleared qf_encoder fusion networks")
        
        if hasattr(self, 'qf_twin_encoder') and hasattr(self.qf_twin_encoder, 'fusion_networks'):
            self.qf_twin_encoder.fusion_networks.clear()
            worker_logger.warning("Cleared qf_twin_encoder fusion networks")
        
        # FIX: Clear any module buffers that might accumulate
        for module in self.modules():
            if hasattr(module, '_buffers'):
                # Check for high-version buffers
                for name, buffer in module._buffers.items():
                    if buffer is not None and hasattr(buffer, '_version') and buffer._version > 10:
                        worker_logger.warning(f"High version buffer found: {name} version {buffer._version}")
                        # Replace with fresh buffer
                        module._buffers[name] = buffer.detach().clone()
        
        # FIX: Aggressive garbage collection
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        
        # DEBUG: Check parameter versions after clear
        high_version_params = []
        for name, param in self.named_parameters():
            if hasattr(param, '_version') and param._version > 15:
                high_version_params.append((name, param._version))
        
        if high_version_params:
            worker_logger.warning(f"High version parameters after cache clear: {high_version_params}")
        
        worker_logger.warning(
            "Episode cache clear complete with nuclear option",
            extra={'worker_id': get_ray_worker_id()}
        )


    def _create_content_hash(self, obs_dict):
        """Create hash based on observation content"""
        hash_parts = []
        for key in sorted(obs_dict.keys()):
            if isinstance(obs_dict[key], torch.Tensor):
                # Use shape and a sample of values for hash
                tensor = obs_dict[key]
                hash_parts.append(f"{key}:{tensor.shape}:{tensor.flatten()[:10].sum().item()}")
            else:
                hash_parts.append(f"{key}:{str(obs_dict[key])[:50]}")
        return hash("_".join(hash_parts))
    def debug_action_space(self, actions):
        """Debug action space configuration"""
        print(f"Action tensor shape: {actions.shape}")
        print(f"Action component sizes: {self.action_component_sizes}")
        print(f"Actions min/max per dimension:")
        for i in range(actions.shape[1]):
            dim_actions = actions[:, i]
            print(f"  Dim {i}: min={dim_actions.min().item()}, max={dim_actions.max().item()}, expected_size={self.action_component_sizes[i] if i < len(self.action_component_sizes) else 'N/A'}")
        
        # Print first few action vectors
        print(f"First 5 action vectors:")
        for i in range(min(5, actions.shape[0])):
            print(f"  {actions[i].tolist()}")
    def _get_cached_tensor(self, key, shape, dtype, device, max_cache_size=100):
        """Create fresh tensors instead of cached ones to avoid in-place modifications"""
        # Don't cache tensors that will be modified - just create fresh ones
        return torch.zeros(shape, dtype=dtype, device=device)
    
    def _setup_sac_components(self):
        obs_space = self.config.observation_space
        action_space = self.config.action_space
        
        # Get worker ID for logging
        worker_id = get_ray_worker_id()
        log_extra = {'worker_id': worker_id}
        
        # Calculate obs_size based on structured observation space
        if isinstance(obs_space, spaces.Dict):
            # For structured observations, calculate individual component sizes
            total_obs_size = 0
            self.obs_component_sizes = {}
            self.obs_component_shapes = {}
            
            # Process each component in the structured observation
            for key, component_space in obs_space.spaces.items():
                if hasattr(component_space, 'shape') and component_space.shape:
                    component_size = int(np.prod(component_space.shape))
                    self.obs_component_sizes[key] = component_size
                    self.obs_component_shapes[key] = component_space.shape
                    total_obs_size += component_size
                else:
                    # Fallback for components without shape
                    self.obs_component_sizes[key] = 100
                    self.obs_component_shapes[key] = (100,)
                    total_obs_size += 100
            
            obs_size = int(total_obs_size)

        else:
            # Fallback to flattened observation space
            if hasattr(obs_space, "shape") and obs_space.shape:
                obs_size = int(np.prod(obs_space.shape))
            else:
                obs_size = 1000  # Conservative fallback
            self.obs_component_sizes = None
            self.obs_component_shapes = None
        
        # Store obs_size as instance variable
        self.obs_size = obs_size
        
        worker_logger.warning(f"Setting up hybrid continuous/discrete RL module for {obs_size}D observations", extra=log_extra)
        
        # Get MLP config and log dimensions
        mlp_config = self._get_stable_mlp_config(obs_size)
        worker_logger.warning(f"MLP config: layers={mlp_config['layers']}, final_dim={mlp_config['final_dim']}", extra=log_extra)
        worker_logger.warning(f"Observation size: {obs_size}, MLP final dim: {mlp_config['final_dim']}", extra=log_extra)
        
        # Get action configuration from constants
        from .constants import get_strategy_action_space
        
        # Get strategy name from config or use default
        # Try dict-like access first (new RLlib API)
        if hasattr(self.config, "get"):
            model_config = self.config.get("model_config", {})
        else:
            model_config = getattr(self.config, "model_config", {})

        if "strategy_name" not in model_config or not model_config["strategy_name"]:
            raise ValueError("No strategy_name provided in model_config!")
        strategy_name = model_config["strategy_name"]

        self.action_config = get_strategy_action_space(strategy_name)
        
        # Extract timing and parameter configurations
        self.timing_action_size = self.action_config['timing']['n']
        self.parameter_configs = self.action_config['parameters']
        
        # Count total parameters for logging
        total_parameters = len(self.parameter_configs)
        continuous_parameters = sum(1 for p in self.parameter_configs if p['type'] == 'continuous')
        discrete_parameters = sum(1 for p in self.parameter_configs if p['type'] == 'discrete')
        
        worker_logger.warning(f"Timing action size: {self.timing_action_size}", extra=log_extra)
        worker_logger.warning(f"Total parameters: {total_parameters} ({continuous_parameters} continuous, {discrete_parameters} discrete)", extra=log_extra)
        
        # Create action component sizes for compatibility with existing code
        self.action_component_sizes = [self.timing_action_size]
        for param_config in self.parameter_configs:
            if param_config['type'] == 'discrete':
                self.action_component_sizes.append(param_config['n'])
            else:
                # For continuous parameters, we treat them as single actions in the discrete representation
                self.action_component_sizes.append(1)
        
        worker_logger.warning(f"Action component sizes: {self.action_component_sizes}", extra=log_extra)
        
        # Create additional attributes for compatibility
        self.num_heads = len(self.action_component_sizes)
        self.mask_keys = [f"head_{i}" for i in range(self.num_heads)]  # Generic mask keys
        
        # Create mask indices for action masking
        self.mask_indices = [0]
        cumulative = 0
        for size in self.action_component_sizes:
            cumulative += size
            self.mask_indices.append(cumulative)
        
        # Structure action sizes (all except timing)
        self.structure_action_sizes = self.action_component_sizes[1:]
        
        # Gradient stabilization parameters - MOVE BEFORE Q-function creation
        self.gradient_clip_norm = 1.0
        self.feature_scale = 0.1  # Scale down intermediate features
        self.residual_scale = 0.5  # Scale residual connections
        
        # Conditional learning parameter - ULTRA-FAST: Enable selective gradient computation
        self.enable_conditional_learning = True
        self.parameter_detach_strength = 0.5  # Add missing parameter detach strength
        
        # Advanced MLP configuration with gradient stability
        mlp_config = self._get_stable_mlp_config(obs_size)
        worker_logger.warning(f"Stable MLP config: {mlp_config}", extra=log_extra)
        
        # Create separate component encoders for structured observations FIRST
        if self.obs_component_sizes and isinstance(obs_space, spaces.Dict):
            self.component_encoders = nn.ModuleDict()
            component_feature_dims = {}
            
            for component_name, component_size in self.obs_component_sizes.items():
                if component_name == "action_mask":
                    continue  # Skip action mask - it's not processed through encoders
                
                # Check if component shapes are available
                if self.obs_component_shapes is None:
                    # Fallback to basic encoder
                    encoder = self._create_basic_encoder(component_size)
                    feature_dim = 32
                # Create specialized encoder for each component based on its nature
                elif component_name == "option_chain":
                    # Option chain needs specialized processing for strikes/expirations
                    encoder = self._create_option_chain_encoder(self.obs_component_shapes[component_name])
                    feature_dim = 256  # Output feature dimension
                elif component_name == "market":
                    # Market data encoder
                    encoder = self._create_market_encoder(component_size)
                    feature_dim = 64
                elif component_name == "portfolio":
                    # Portfolio encoder
                    encoder = self._create_portfolio_encoder(component_size)
                    feature_dim = 64
                elif component_name == "positions":
                    # Positions encoder with attention over positions
                    if self.obs_component_shapes is not None:
                        encoder = self._create_positions_encoder(self.obs_component_shapes[component_name])
                    else:
                        encoder = self._create_basic_encoder(component_size)
                    feature_dim = 128
                elif component_name == "environment":
                    # Environment state encoder
                    encoder = self._create_environment_encoder(component_size)
                    feature_dim = 32
                else:
                    # Generic encoder for other components
                    encoder = self._create_generic_component_encoder(component_size)
                    feature_dim = 64
                
                self.component_encoders[component_name] = encoder
                component_feature_dims[component_name] = feature_dim
            import torch.utils.checkpoint as checkpoint
            if hasattr(self, 'component_encoders'):
                for name, encoder in self.component_encoders.items():
                    self.component_encoders[name] = (encoder)
                
                worker_logger.warning(f"Applied gradient checkpointing to {len(self.component_encoders)} component encoders", 
                                    extra={'worker_id': get_ray_worker_id()})
            
            # Total feature dimension after component encoding
            total_component_features = sum(component_feature_dims.values())
            
            # Create fusion network to combine component features - FIX: Use 544 to match concatenated features
            self.component_fusion = self._create_component_fusion_network(
                component_feature_dims, 544
            )
            
            # Set the encoder to use component-based processing
            self.mlp_encoder = None  # We'll use component encoders instead
            self.component_feature_dims = component_feature_dims
            
            worker_logger.warning(f"Created component encoders: {list(self.component_encoders.keys())}", extra=log_extra)
            worker_logger.warning(f"Component feature dimensions: {component_feature_dims}", extra=log_extra)
        else:
            # Fallback to traditional MLP encoder for flattened observations
            self.mlp_encoder = self._create_stable_mlp_encoder(obs_size, mlp_config)
            self.component_encoders = None
            self.component_fusion = None
            self.component_feature_dims = None
        
        # Calculate total action dimension for Q-functions
        total_action_dim = self.timing_action_size + sum(
            2 if p['type'] == 'continuous' else p['n'] 
            for p in self.parameter_configs
        )
        
        # CRITICAL: Create Q-function encoders FIRST - REQUIRED NAMES FOR SAC
        def make_stable_q_encoder():
            if hasattr(self, 'component_encoders') and self.component_encoders:
                return self._create_stable_component_q_encoder(obs_space, total_action_dim)
            else:
                return self._create_stable_q_encoder(obs_size, total_action_dim)
        
        self.qf_encoder = make_stable_q_encoder()
        self.qf_twin_encoder = make_stable_q_encoder()

        worker_logger.warning("Applied gradient checkpointing to Q-function encoders", 
                            extra={'worker_id': get_ray_worker_id()})
        # Q-function heads
        self.qf = self._create_stable_q_head()
        self.qf_twin = self._create_stable_q_head()
        
        # Target networks - CREATE FRESH INSTANCES (don't copy state dict yet)
        self.qf_target = self._create_stable_q_head()
        self.qf_twin_target = self._create_stable_q_head()
        self.qf_target_encoder = make_stable_q_encoder()
        self.qf_twin_target_encoder = make_stable_q_encoder()
        
        # Target encoders - REQUIRED FOR SAC
        if hasattr(self, 'component_encoders') and self.component_encoders:
            self.qf_target_encoder = self._create_stable_component_q_encoder(obs_space, total_action_dim)
            self.qf_twin_target_encoder = self._create_stable_component_q_encoder(obs_space, total_action_dim)
        else:
            self.qf_target_encoder = make_stable_q_encoder()
            self.qf_twin_target_encoder = make_stable_q_encoder()
        
        # Policy encoder (convenience wrapper) - REQUIRED FOR SAC COMPATIBILITY
        if hasattr(self, 'component_encoders') and self.component_encoders:
            # ULTRA-FAST: Create a wrapper encoder that uses component encoders for SAC compatibility
            class ComponentPolicyEncoder(nn.Module):
                def __init__(self, component_encoders, component_fusion):
                    super().__init__()
                    self.component_encoders = component_encoders
                    self.component_fusion = component_fusion
                
                def forward(self, obs_dict):
                    """Forward pass for component-based encoding"""
                    if not isinstance(obs_dict, dict) or not self.component_encoders:
                        # Fallback for non-dict observations
                        if isinstance(obs_dict, torch.Tensor):
                            return obs_dict
                        else:
                            return torch.zeros(1, 384, dtype=torch.float32)
                    
                    component_features = []
                    component_order = ["option_chain", "market", "portfolio", "positions", "environment"]
                    
                    for component_name in component_order:
                        if component_name in obs_dict and component_name in self.component_encoders:
                            component_data = obs_dict[component_name]
                            
                            # ULTRA-FAST: Use as_tensor for speed
                            if not isinstance(component_data, torch.Tensor):
                                component_data = torch.as_tensor(component_data, dtype=torch.float32)
                            
                            # Apply component encoder
                            try:
                                component_features_encoded = self.component_encoders[component_name](component_data)
                                component_features.append(component_features_encoded)
                            except Exception:
                                # Fallback dummy features
                                batch_size = component_data.shape[0] if component_data.dim() > 0 else 1
                                dummy_features = torch.zeros(batch_size, 64, dtype=torch.float32, device=component_data.device)
                                component_features.append(dummy_features)
                    
                    if component_features:
                        concatenated_features = torch.cat(component_features, dim=-1)
                        
                        worker_logger.warning(f"[DEBUG] ComponentPolicyEncoder concatenated features shape: {concatenated_features.shape}")
                        
                        if self.component_fusion:
                            try:
                                result = self.component_fusion(concatenated_features)
                                worker_logger.warning(f"[DEBUG] ComponentPolicyEncoder fusion result shape: {result.shape}")
                                return result
                            except Exception as e:
                                worker_logger.error(f"[DEBUG] ComponentPolicyEncoder fusion failed: {e}",
                                                   extra={'worker_id': get_ray_worker_id()})
                                worker_logger.warning(f"[DEBUG] ComponentPolicyEncoder fallback to concatenated features: {concatenated_features.shape}")
                                return concatenated_features
                        else:
                            worker_logger.warning(f"[DEBUG] ComponentPolicyEncoder no fusion, returning concatenated: {concatenated_features.shape}")
                            return concatenated_features
                    else:
                        return torch.zeros(1, 384, dtype=torch.float32)
            
            # CRITICAL: SAC requires pi_encoder attribute - create wrapper
            self.pi_encoder = ComponentPolicyEncoder(self.component_encoders, self.component_fusion)
            # For component encoders, the output dimension is determined by the fusion network - FIX: Use 544
            policy_feature_dim = 544  # Fusion network outputs 544 to match concatenated features
        else:
            # Traditional encoder - REQUIRED FOR SAC
            self.pi_encoder = self.mlp_encoder
            # For traditional encoder, use the MLP config final dimension
            policy_feature_dim = mlp_config['final_dim']
        
        worker_logger.warning(f"Creating policy trunk with feature dimension: {policy_feature_dim}", extra=log_extra)
        worker_logger.warning(f"Using component encoders: {self.component_encoders is not None}", extra=log_extra)
        worker_logger.warning(f"MLP config final_dim: {mlp_config['final_dim']}", extra=log_extra)
        self.policy_trunk = self._create_stable_policy_trunk(policy_feature_dim)
        
        # Create individual heads for timing and each parameter
        worker_logger.warning(f"Creating action heads with input dimension: {policy_feature_dim}", extra=log_extra)
        self.timing_head = self._create_stable_action_head(policy_feature_dim, self.timing_action_size)
        
        # Parameter heads - separate for each parameter
        self.parameter_heads = nn.ModuleDict()
        for param_config in self.parameter_configs:
            param_name = param_config['name']
            if param_config['type'] == 'continuous':
                output_size = 2
            else:
                output_size = param_config['n']
            
            # Use policy_feature_dim to match policy trunk output
            self.parameter_heads[param_name] = self._create_stable_action_head(
                policy_feature_dim, output_size
            )
        
        # CREATE PI ATTRIBUTE FOR SAC COMPATIBILITY
        class HybridPolicyNetwork(nn.Module):
            def __init__(self, encoder, trunk, timing_head, parameter_heads, parameter_configs, obs_component_sizes, feature_scale=0.1, component_encoders=None, component_fusion=None, obs_component_shapes=None):
                super().__init__()
                self.encoder = encoder
                self.trunk = trunk
                self.timing_head = timing_head
                self.parameter_heads = parameter_heads
                self.parameter_configs = parameter_configs
                self.obs_component_sizes = obs_component_sizes
                self.obs_component_shapes = obs_component_shapes
                self.feature_scale = feature_scale
                
                # Component-based processing
                self.component_encoders = component_encoders
                self.component_fusion = component_fusion
                self.use_component_encoding = component_encoders is not None
            
            def forward(self, x):
                # ULTRA-FAST: Use encoder directly for all cases (handles both component and traditional)
                if isinstance(x, dict):
                    # For structured observations, use encoder's forward method
                    features = self.encoder(x) * self.feature_scale
                else:
                    # For non-dict observations, normalize first then encode
                    x_norm = self._normalize_input(x)
                    features = self.encoder(x_norm) * self.feature_scale
                
                trunk_out = self.trunk(features)
                
                # Timing head output (discrete)
                timing_logits = self.timing_head(trunk_out)
                if timing_logits.requires_grad:
                    timing_logits.register_hook(lambda grad: torch.clamp(grad, -1.0, 1.0))
                
                # Parameter head outputs (mixed continuous/discrete)
                param_outputs = [timing_logits]
                for param_config in self.parameter_configs:
                    param_name = param_config['name']
                    param_output = self.parameter_heads[param_name](trunk_out)
                    if param_output.requires_grad:
                        param_output.register_hook(lambda grad: torch.clamp(grad, -1.0, 1.0))
                    param_outputs.append(param_output)
                
                return torch.cat(param_outputs, dim=-1)
            
            def _process_structured_obs_with_components(self, obs_dict):
                """Process structured observations using component encoders (NO FLATTENING)"""
                worker_id = get_ray_worker_id()
                log_extra = {'worker_id': worker_id}
                
                if not isinstance(obs_dict, dict) or not self.component_encoders:
                    return obs_dict
                
                component_features = []
                component_dims = {}
                
                # Process each component with its specialized encoder
                component_order = ["option_chain", "market", "portfolio", "positions", "environment"]
                
                for component_name in component_order:
                    if component_name in obs_dict and component_name in self.component_encoders:
                        component_data = obs_dict[component_name]
                        
                        # ULTRA-FAST: Use as_tensor instead of tensor for speed
                        if not isinstance(component_data, torch.Tensor):
                            component_data = torch.as_tensor(component_data, dtype=torch.float32)
                        
                        # ULTRA-FAST: Cache shape lookups and avoid repeated dictionary access
                        if component_name == "market":
                            expected_shape = (10,)  # Force 10 features
                        else:
                            expected_shape = self.obs_component_shapes.get(component_name) if self.obs_component_shapes is not None else None

                        if expected_shape and component_data.shape[-len(expected_shape):] != expected_shape:
                            # Check if reshaping is mathematically possible
                            if component_data.dim() > len(expected_shape):
                                batch_size = component_data.shape[0]
                                actual_feature_size = int(np.prod(component_data.shape[1:]))
                                expected_total = int(np.prod(expected_shape))
                                
                                if actual_feature_size == expected_total:
                                    component_data = component_data.view(batch_size, *expected_shape)
                        
                        # Apply component-specific encoder
                        try:
                            # FIX: Ensure dtype consistency for mixed precision - check all parameters and biases
                            encoder = self.component_encoders[component_name]
                            target_dtype = torch.float32  # Default to float32
                            target_device = component_data.device
                            
                            # Check all parameters in the encoder to find the most common dtype
                            param_dtypes = []
                            for param in encoder.parameters():
                                param_dtypes.append(param.dtype)
                            
                            if param_dtypes:
                                # Use the first parameter's dtype (they should all be consistent)
                                target_dtype = param_dtypes[0]
                                target_device = next(encoder.parameters()).device
                                
                                # Convert input to match encoder parameters
                                component_data = component_data.to(dtype=target_dtype, device=target_device)
                            
                            component_features_encoded = encoder(component_data)
                            
                            # Convert output back to float32 for concatenation consistency
                            if component_features_encoded.dtype != torch.float32:
                                component_features_encoded = component_features_encoded.to(torch.float32)
                            
                            component_features.append(component_features_encoded)
                            component_dims[component_name] = component_features_encoded.shape[-1]
                        except Exception as e:
                            worker_logger.error(f"[ERROR] Encoder failed for '{component_name}': {e}", extra=log_extra)
                            # Fallback for component encoding errors
                            # ULTRA-FAST: Pre-allocate dummy features
                            batch_size = component_data.shape[0] if component_data.dim() > 0 else 1
                            dummy_features = torch.zeros(batch_size, 64, dtype=torch.float32, device=component_data.device)
                            component_features.append(dummy_features)
                            component_dims[component_name] = 64
                
                if component_features:
                    # ULTRA-FAST: Fuse component features with pre-computed dimensions
                    concatenated_features = torch.cat(component_features, dim=-1)
                    
                    # ULTRA-FAST: Cache dimension checks
                    if hasattr(self.component_fusion, 'component_feature_dims'):
                        expected_dim = sum(self.component_fusion.component_feature_dims.values())
                        actual_dim = concatenated_features.shape[-1]
                        
                        if self.component_fusion and actual_dim == expected_dim:
                            try:
                                fused_features = self.component_fusion(concatenated_features)
                                return fused_features
                            except Exception as e:
                                worker_logger.error(f"[ERROR] Fusion network failed: {e}", extra=log_extra)
                                # Fallback: return concatenated features if fusion fails
                                return concatenated_features
                    
                    # Dimension mismatch or no fusion network - return concatenated features
                    return concatenated_features
                else:
                    # ULTRA-FAST: Pre-allocate dummy features with cached batch size
                    batch_size = 1
                    for value in obs_dict.values():
                        if isinstance(value, torch.Tensor) and value.dim() > 0:
                            batch_size = value.shape[0]
                            break
                    
                    dummy_result = torch.zeros(batch_size, 384, dtype=torch.float32)  # Default feature size
                    return dummy_result
            
            def _process_structured_obs(self, obs_dict):
                """Process structured observation dict while maintaining relationships (FALLBACK WITH FLATTENING)"""
                if isinstance(obs_dict, dict):
                    processed_components = []
                    
                    # Process components in a specific order to maintain consistency
                    component_order = ["option_chain", "market", "portfolio", "positions", "environment"]
                    
                    for key in component_order:
                        if key in obs_dict:
                            component = obs_dict[key]
                            
                            # ULTRA-FAST: Use as_tensor instead of tensor for speed
                            if not isinstance(component, torch.Tensor):
                                component = torch.as_tensor(component, dtype=torch.float32)
                            
                            # ULTRA-FAST: Handle the component based on its structure - cached processing
                            if key == "option_chain":
                                # Special handling for option chain to preserve strike/expiry relationships
                                processed_component = self._process_option_chain(component)
                            elif key == "market":
                                # Market data processing
                                processed_component = self._process_market_data(component)
                            elif key == "portfolio":
                                # Portfolio processing
                                processed_component = self._process_portfolio_data(component)
                            elif key == "positions":
                                # Position data processing
                                processed_component = self._process_position_data(component)
                            elif key == "environment":
                                # Environment state processing
                                processed_component = self._process_environment_data(component)
                            else:
                                # Generic processing for other components
                                processed_component = self._process_generic_component(component)
                            
                            processed_components.append(processed_component)
                    
                    if processed_components:
                        # ULTRA-FAST: Concatenate all processed components
                        return torch.cat(processed_components, dim=-1)
                    else:
                        # ULTRA-FAST: Pre-allocated fallback tensor
                        return torch.zeros(1, 1000, dtype=torch.float32)
                else:
                    # Already flattened or non-dict input
                    return obs_dict
            
            def _process_option_chain(self, option_chain):
                """Process option chain while preserving strike/expiry structure"""
                if option_chain.dim() > 1:
                    # If multi-dimensional, preserve the first dimension and flatten others selectively
                    batch_size = option_chain.shape[0]
                    # Reshape to preserve batch dimension
                    flattened = option_chain.view(batch_size, -1)
                else:
                    flattened = option_chain.flatten()
                return flattened
            
            def _process_market_data(self, market_data):
                """Process market data preserving key relationships"""
                if market_data.dim() > 1:
                    batch_size = market_data.shape[0]
                    flattened = market_data.view(batch_size, -1)
                else:
                    flattened = market_data.flatten()
                return flattened
            
            def _process_portfolio_data(self, portfolio_data):
                """Process portfolio data"""
                if portfolio_data.dim() > 1:
                    batch_size = portfolio_data.shape[0]
                    flattened = portfolio_data.view(batch_size, -1)
                else:
                    flattened = portfolio_data.flatten()
                return flattened
            
            def _process_position_data(self, position_data):
                """Process position data"""
                if position_data.dim() > 1:
                    batch_size = position_data.shape[0]
                    flattened = position_data.view(batch_size, -1)
                else:
                    flattened = position_data.flatten()
                return flattened
            
            def _process_environment_data(self, env_data):
                """Process environment state data"""
                if env_data.dim() > 1:
                    batch_size = env_data.shape[0]
                    flattened = env_data.view(batch_size, -1)
                else:
                    flattened = env_data.flatten()
                return flattened
            
            def _process_generic_component(self, component):
                """Generic component processing"""
                if component.dim() > 1:
                    batch_size = component.shape[0]
                    flattened = component.view(batch_size, -1)
                else:
                    flattened = component.flatten()
                return flattened
            
            def _normalize_input(self, obs):
                """Wrapper for JIT compiled normalization"""
                return _normalize_input_fast(obs)
        
        self.pi = HybridPolicyNetwork(
            encoder=self.pi_encoder,  # ULTRA-FAST: Use pi_encoder (works for both component and traditional)
            trunk=self.policy_trunk,
            timing_head=self.timing_head,
            parameter_heads=self.parameter_heads,
            parameter_configs=self.parameter_configs,
            obs_component_sizes=self.obs_component_sizes,
            obs_component_shapes=self.obs_component_shapes,
            feature_scale=self.feature_scale,
            component_encoders=self.component_encoders,
            component_fusion=self.component_fusion
        )
        
        # Value head with gradient stability
        self.value_head = self._create_stable_value_head(policy_feature_dim)
        
        # Alpha parameter with gradient clipping
        self.log_alpha = nn.Parameter(torch.tensor(0.0))
        
        # Masking configuration - adapted for hybrid actions
        self.inv_temperature = 1.0 / 0.9
        self.mask_min = -1.0e2  # Less extreme masking to prevent gradient issues
        
        # Initialize weights with gradient-stable initialization
        self._init_weights_manually()
        self._setup_enhanced_mixed_precision()
        self._setup_cuda_streams()



        # THEN initialize target networks (after weights are set)
        try:
            self.qf_target.load_state_dict(self.qf.state_dict())
            self.qf_twin_target.load_state_dict(self.qf_twin.state_dict())
            # Skip encoder state dict copying for dynamic encoders
            if not (hasattr(self, 'component_encoders') and self.component_encoders):
                self.qf_target_encoder.load_state_dict(self.qf_encoder.state_dict())
                self.qf_twin_target_encoder.load_state_dict(self.qf_twin_encoder.state_dict())
        except Exception as e:
            worker_logger.warning(f"Target network initialization failed: {e}, continuing...", 
                                extra={'worker_id': get_ray_worker_id()})
        # Initialize target networks
        self.qf_target.load_state_dict(self.qf.state_dict())
        self.qf_twin_target.load_state_dict(self.qf_twin.state_dict())
        
        # View requirements - initialize dict first
        self.view_requirements = {}
        self.view_requirements["obs_next"] = ViewRequirement(
            data_col="obs", shift=1, used_for_training=True
        )
        
        # Register gradient clipping hooks
        self._register_gradient_hooks()
        
        worker_logger.warning("Hybrid continuous/discrete RL module setup complete", extra=log_extra)
    def _batch_process_components(self, obs_dict_list):
        """Process multiple observation dictionaries in parallel"""
        if len(obs_dict_list) == 1:
            return [self.pi._process_structured_obs_with_components(obs_dict_list[0])]
        
        def process_single_obs(obs_dict):
            return self.pi._process_structured_obs_with_components(obs_dict)
        
        with ThreadPoolExecutor(max_workers=min(len(obs_dict_list), self.parallel_workers)) as executor:
            futures = [executor.submit(process_single_obs, obs_dict) for obs_dict in obs_dict_list]
            results = [future.result() for future in futures]
        
        return results
    
    def load_state_dict(self, state_dict, strict=True):
        """Load state dict with backwards compatibility for market state expansion from 6 to 10 features"""
        # Get worker logger
        worker_id = get_ray_worker_id()
        log_extra = {'worker_id': worker_id}
        
        try:
            # Create a compatible state dict by handling size mismatches
            adapted_state_dict = {}
            
            for key, value in state_dict.items():
                if 'market' in key and isinstance(value, torch.Tensor):
                    # Handle market encoder size mismatch (6 -> 10 features)
                    if value.shape == torch.Size([32, 6]) and hasattr(self, 'component_encoders'):
                        # This is likely the first layer of market encoder expecting 6 features
                        # We need to expand it to 10 features
                        worker_logger.warning(f"Adapting market encoder layer {key} from 6 to 10 features", extra=log_extra)
                        
                        # Create new weight tensor with 10 input features
                        new_weight = torch.zeros(32, 10, dtype=value.dtype)
                        # Copy original weights for first 6 features
                        new_weight[:, :6] = value
                        # Initialize new features (7-10) with small random values
                        new_weight[:, 6:] = torch.randn(32, 4, dtype=value.dtype) * 0.01
                        adapted_state_dict[key] = new_weight
                    else:
                        adapted_state_dict[key] = value
                else:
                    adapted_state_dict[key] = value
            
            # Load the adapted state dict
            return super().load_state_dict(adapted_state_dict, strict=False)
            
        except Exception as e:
            worker_logger.warning(f"State dict loading failed: {e}, trying strict=False", extra=log_extra)
            # Fallback to non-strict loading
            return super().load_state_dict(state_dict, strict=False)
    
    def _apply_conditional_gradient_scaling(self, tensor, scale_factor):
        """Apply gradient scaling using a custom autograd function"""
        if not self.enable_conditional_learning or scale_factor >= 1.0:
            return tensor
        
        # Custom gradient scaling function
        class GradientScaler(torch.autograd.Function):
            @staticmethod
            def forward(ctx, input_tensor, scale):
                ctx.scale = scale
                return input_tensor
            
            @staticmethod
            def backward(ctx, grad_output):
                return grad_output * ctx.scale, None
        
        return GradientScaler.apply(tensor, scale_factor)
    
    def _get_position_state_from_observations(self, observations):
        """Extract position state from structured observations"""
        has_open_position = False
        
        if isinstance(observations, dict):
            # Method 1: Check environment state
            if "environment" in observations:
                env_state = observations["environment"]
                if hasattr(env_state, '__len__') and len(env_state) > 0:
                    env_flat = env_state.flatten() if hasattr(env_state, 'flatten') else env_state
                    if len(env_flat) > 0:
                        has_open_position = bool(env_flat[0] > 0)
            
            # Method 2: Check positions tensor as fallback
            if not has_open_position and "positions" in observations:
                positions = observations["positions"]
                if hasattr(positions, '__len__') and len(positions) > 0:
                    pos_flat = positions.flatten() if hasattr(positions, 'flatten') else positions
                    # Check if any position has non-zero values (indicating active positions)
                    if len(pos_flat) > 0:
                        has_open_position = bool(torch.any(torch.abs(pos_flat) > 1e-6))
        
        return has_open_position
    
    def _get_stable_mlp_config(self, obs_size: int) -> dict:
        """Gradient-stable MLP architecture for high-dimensional observations"""
        # Calculate reasonable hidden dimensions based on observation size
        if obs_size <= 500:
            # Small observation spaces
            hidden_dims = [obs_size, min(256, obs_size * 2), min(128, obs_size)]
            final_dim = min(128, obs_size)
        elif obs_size <= 1500:
            # Medium observation spaces
            hidden_dims = [obs_size, min(512, obs_size), min(256, obs_size // 2)]
            final_dim = min(256, obs_size // 2)
        else:
            # Large observation spaces
            hidden_dims = [obs_size, min(1024, obs_size), min(768, obs_size // 2), min(480, obs_size // 4)]
            final_dim = min(480, obs_size // 4)
        
        return {
            'layers': hidden_dims,
            'final_dim': final_dim,  # Dynamic final dimension based on obs_size
            'use_residual': True,
            'use_attention': False,  # Disable attention for stability
            'use_layer_scale': True,  # Add layer scaling
            'dropout_schedule': [0.05, 0.1, 0.1, 0.05]  # Progressive dropout
        }
    
    def _create_stable_policy_trunk(self, input_dim: int) -> nn.Module:
        """Create policy trunk with gradient stability"""
        worker_logger.warning(f"Creating stable policy trunk with input dimension: {input_dim}", extra={'worker_id': get_ray_worker_id()})
        return nn.Sequential(
            nn.Linear(input_dim, input_dim),           # 480 -> 480
            LayerScale(input_dim),                     # LayerScale(480)
            nn.LayerNorm(input_dim),                   # LayerNorm(480)
            nn.GELU(),
            nn.Dropout(0.05),
            StableResidualBlock(input_dim, input_dim // 2, 0.1, self.residual_scale),  # 480 -> 480 (fixed)
            nn.LayerNorm(input_dim),                   # FIX: LayerNorm(480) not 240
            GradientClipActivation(),
        )
    def _create_stable_action_head(self, input_dim: int, output_dim: int) -> nn.Module:
        """Create action head with gradient stability"""
        return nn.Sequential(
            StableResidualBlock(input_dim, input_dim, 0.05, self.residual_scale),
            nn.Linear(input_dim, max(input_dim // 2, output_dim)),
            nn.LayerNorm(max(input_dim // 2, output_dim)),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(max(input_dim // 2, output_dim), output_dim, bias=False),
            # Apply gradient clipping to final layer
            GradientClipLayer()
        )
    
    def _create_stable_value_head(self, input_dim: int) -> nn.Module:
        """Create value head with gradient stability"""
        return nn.Sequential(
            StableResidualBlock(input_dim, input_dim, 0.05, self.residual_scale),
            nn.Linear(input_dim, input_dim // 2),
            LayerScale(input_dim // 2),
            nn.LayerNorm(input_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim // 2, input_dim // 4),
            nn.LayerNorm(input_dim // 4),
            nn.GELU(),
            nn.Linear(input_dim // 4, 1, bias=False),
            GradientClipLayer()
        )

    def _create_stable_mlp_encoder(self, obs_size: int, config: dict) -> nn.Module:
        """Create gradient-stable MLP encoder with multiple stability techniques"""
        layers = config['layers']
        use_residual = config['use_residual']
        use_layer_scale = config['use_layer_scale']
        dropout_schedule = config['dropout_schedule']
        
        modules = []
        
        # Input normalization and projection
        modules.append(nn.LayerNorm(layers[0]))
        modules.append(nn.Linear(layers[0], layers[1]))
        if use_layer_scale:
            modules.append(LayerScale(layers[1]))
        modules.append(nn.LayerNorm(layers[1]))
        modules.append(nn.GELU())
        modules.append(nn.Dropout(dropout_schedule[0]))
        
        # Hidden layers with gradient-stable residual connections
        for i in range(1, len(layers) - 1):
            in_dim, out_dim = layers[i], layers[i + 1]
            dropout_rate = dropout_schedule[min(i, len(dropout_schedule) - 1)]
            
            if use_residual and in_dim == out_dim:
                modules.append(StableResidualBlock(in_dim, out_dim, dropout_rate, self.residual_scale))
            else:
                modules.append(nn.Linear(in_dim, out_dim))
                if use_layer_scale:
                    modules.append(LayerScale(out_dim))
                modules.append(nn.LayerNorm(out_dim))
                modules.append(nn.GELU())
                modules.append(nn.Dropout(dropout_rate))
        
        # Final projection with gradient clipping
        modules.append(nn.Linear(layers[-1], config['final_dim']))
        modules.append(nn.LayerNorm(config['final_dim']))
        modules.append(GradientClipActivation())
        
        return nn.Sequential(*modules)

    def _create_stable_q_encoder(self, obs_size: int, action_dim: int) -> nn.Module:
        """Create Q-function encoder with gradient stability"""
        input_dim = obs_size + action_dim
        
        return nn.Sequential(
            nn.LayerNorm(input_dim),
            nn.Linear(input_dim, 1024),
            LayerScale(1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            StableResidualBlock(1024, 1024, 0.1, self.residual_scale),
            nn.Linear(1024, 512),
            LayerScale(512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            GradientClipActivation()
        )
    def _create_stable_component_q_encoder(self, obs_space: spaces.Dict, action_dim: int) -> nn.Module:
        """Create Q-function encoder for component-based observations - FIXED MEMORY LEAK"""
        
        class ComponentQEncoder(nn.Module):
            def __init__(self, action_dim):
                super().__init__()
                self.action_dim = action_dim
                
                # Action processing layer
                self.action_processor = nn.Sequential(
                    nn.Linear(action_dim, 64),
                    nn.LayerNorm(64),
                    nn.GELU()
                )
                
                # Pre-create fusion networks for common dimensions to avoid dynamic creation
                self.fusion_networks = nn.ModuleDict()
                self.action_feature_dim = 64
                
                # Pre-create for common obs dimensions based on your logs
                common_dims = [2902, 476, 477, 1000, 2000, 3000, 2916, 2951]
                for dim in common_dims:
                    self.fusion_networks[str(dim)] = self._create_fusion_network(dim)
                
                # Track which dimensions we've seen to prevent repeated warnings
                self._seen_dimensions = set(common_dims)
            
            def _create_fusion_network(self, obs_dim):
                """Create fusion network with correct dimensions"""
                total_input_dim = obs_dim + self.action_feature_dim
                
                return nn.Sequential(
                    nn.Linear(total_input_dim, 512),
                    LayerScale(512),
                    nn.LayerNorm(512),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(512, 256),
                    nn.LayerNorm(256),
                    GradientClipActivation()
                )
            
            def forward(self, obs_action_concat):
                # CRITICAL: Ensure input is on correct device
                obs_action_concat = obs_action_concat.to(self.action_processor[0].weight.device)
                
                # Split obs and action
                obs_features = obs_action_concat[:, :-self.action_dim]
                action_features = obs_action_concat[:, -self.action_dim:]
                
                # Ensure consistent dtype for action processing
                action_features = action_features.float()
                obs_features = obs_features.float()
                
                # Process action
                action_processed = self.action_processor(action_features)
                
                # Combine obs and action features
                combined = torch.cat([obs_features, action_processed], dim=-1)
                
                # Use pre-created fusion network or create if absolutely necessary
                obs_dim = obs_features.shape[-1]
                network_key = str(obs_dim)
                
                if network_key in self.fusion_networks:
                    # Use existing pre-created network
                    fusion_network = self.fusion_networks[network_key]
                else:
                    # Only create if absolutely necessary and warn once per dimension
                    if obs_dim not in self._seen_dimensions:
                        worker_id = get_ray_worker_id()
                        worker_logger.warning(
                            f"Creating new fusion network for obs_dim={obs_dim} - consider adding to common_dims. "
                            f"This may cause memory accumulation.",
                            extra={'worker_id': worker_id}
                        )
                        self._seen_dimensions.add(obs_dim)
                    
                    # Create and store the new network
                    fusion_network = self._create_fusion_network(obs_dim)
                    self.fusion_networks[network_key] = fusion_network
                    
                    # CRITICAL: Move to correct device - use the device of the input tensor
                    target_device = combined.device
                    fusion_network = fusion_network.to(target_device)
                    
                    # Limit total networks to prevent unlimited growth
                    if len(self.fusion_networks) > 20:
                        # Remove least recently used networks (keep first 10)
                        keys_to_remove = list(self.fusion_networks.keys())[10:]
                        for key in keys_to_remove:
                            del self.fusion_networks[key]
                        
                        worker_logger.warning(
                            f"Fusion network cache exceeded 20 entries, removed {len(keys_to_remove)} networks",
                            extra={'worker_id': get_ray_worker_id()}
                        )
                
                return fusion_network(combined)
        
        return ComponentQEncoder(action_dim)
        
    def _create_stable_q_head(self) -> nn.Module:
        """Create Q-function head with gradient stability"""
        return nn.Sequential(
            nn.Linear(256, 128),
            LayerScale(128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, 1, bias=False),
            GradientClipLayer()
        )
    
    def _process_components_parallel_parallel(self, obs_dict):
        """Process components in parallel using async execution"""
        if not isinstance(obs_dict, dict) or not self.component_encoders:
            return obs_dict
        
        # Prepare all inputs first
        component_inputs = {}
        for name in ["option_chain", "market", "portfolio", "positions", "environment"]:
            if name in obs_dict and name in self.component_encoders:
                data = obs_dict[name]
                if not isinstance(data, torch.Tensor):
                    data = torch.as_tensor(data, dtype=torch.float32)
                component_inputs[name] = data
        
        # Process all components in parallel streams
        component_features = []
        streams = []
        
        for i, (name, data) in enumerate(component_inputs.items()):
            stream = torch.cuda.Stream() if torch.cuda.is_available() else None
            streams.append(stream)
            
            if stream:
                with torch.cuda.stream(stream):
                    features = self.component_encoders[name](data)
            else:
                features = self.component_encoders[name](data)
            component_features.append(features)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        return torch.cat(component_features, dim=-1)
    def _create_option_chain_encoder(self, shape: tuple) -> nn.Module:
        import torch.nn as nn
        import torch

        num_strikes, num_expirations, num_types, num_features = shape

        class LeanOptionChainEncoder(nn.Module):
            def __init__(self, num_strikes, num_expirations, num_types, num_features):
                super().__init__()
                self.num_strikes = num_strikes
                self.num_expirations = num_expirations
                self.num_types = num_types

                # Reduced channels to save memory
                self.option_type_encoders = nn.ModuleList([
                    nn.Sequential(
                        nn.Conv2d(num_features, 16, kernel_size=1),
                        nn.GroupNorm(4, 16),
                        nn.GELU(),
                        nn.Dropout(0.05),

                        nn.Conv2d(16, 24, kernel_size=(3, 1), padding=(1, 0)),
                        nn.GroupNorm(4, 24),
                        nn.GELU(),

                        nn.Conv2d(24, 32, kernel_size=(1, 3), padding=(0, 1)),
                        nn.GroupNorm(4, 32),
                        nn.GELU(),
                        nn.Dropout(0.1)
                    ) for _ in range(num_types)
                ])

                # Adjusted cross-type fusion
                self.cross_type_fusion = nn.Sequential(
                    nn.Conv2d(32 * num_types, 48, kernel_size=1),
                    nn.GroupNorm(4, 48),
                    nn.GELU(),
                    nn.Dropout(0.1),

                    nn.Conv2d(48, 48, kernel_size=3, padding=1),
                    nn.GroupNorm(4, 48),
                    nn.GELU()
                )

                # Lower model dim and heads
                self.strike_attention = nn.TransformerEncoderLayer(
                    d_model=48, nhead=4, dim_feedforward=128, 
                    dropout=0.1, batch_first=True, norm_first=True
                )
                self.expiry_attention = nn.TransformerEncoderLayer(
                    d_model=48, nhead=4, dim_feedforward=128, 
                    dropout=0.1, batch_first=True, norm_first=True
                )

                self.strike_pos_encoding = nn.Parameter(
                    torch.randn(1, 48, num_strikes, 1) * 0.02
                )
                self.expiry_pos_encoding = nn.Parameter(
                    torch.randn(1, 48, 1, num_expirations) * 0.02
                )

                self.importance_weighting = nn.Sequential(
                    nn.Conv2d(48, 24, kernel_size=1),
                    nn.GELU(),
                    nn.Conv2d(24, 1, kernel_size=1),
                    nn.Sigmoid()
                )

                # Smaller multi-resolution targets
                self.multi_resolution_pools = nn.ModuleList([
                    nn.AdaptiveAvgPool2d((6, 3)),
                    nn.AdaptiveAvgPool2d((3, 2)),
                    nn.AdaptiveAvgPool2d((1, 1)),
                ])
    

    
                 # BUT change the final output to match your working 256:
                self.final_fusion = nn.Sequential(
                    nn.Linear(48 * (6*3 + 3*2 + 1*1), 384),
                    nn.LayerNorm(384),
                    nn.GELU(),
                    nn.Dropout(0.1),

                    nn.Linear(384, 256),
                    nn.LayerNorm(256),
                    nn.GELU(),
                    nn.Dropout(0.1),

                    nn.Linear(256, 256),  # FINAL OUTPUT: 256 (your original)
                    nn.LayerNorm(256),
                    GradientClipActivation()
                )

            def forward(self, x):
                batch_size = x.shape[0]
                type_data_batch = x.permute(0, 3, 4, 1, 2)
                type_data_flat = type_data_batch.reshape(batch_size * self.num_types, num_features, self.num_strikes, self.num_expirations)

                type_features_all = []
                for type_idx in range(self.num_types):
                    start = type_idx * batch_size
                    end = (type_idx + 1) * batch_size
                    type_encoded = self.option_type_encoders[type_idx](type_data_flat[start:end])
                    type_features_all.append(type_encoded)

                combined_features = torch.cat(type_features_all, dim=1)
                fused = self.cross_type_fusion(combined_features)
                fused = fused + self.strike_pos_encoding + self.expiry_pos_encoding

                # Strike attention
                strike = fused.permute(0, 3, 2, 1).reshape(batch_size * self.num_expirations, self.num_strikes, 48)
                strike_attended = self.strike_attention(strike).reshape(batch_size, self.num_expirations, self.num_strikes, 48).permute(0, 3, 2, 1)

                # Expiry attention
                expiry = fused.permute(0, 2, 3, 1).reshape(batch_size * self.num_strikes, self.num_expirations, 48)
                expiry_attended = self.expiry_attention(expiry).reshape(batch_size, self.num_strikes, self.num_expirations, 48).permute(0, 3, 1, 2)

                # Combine
                attention_combined = torch.stack([
                    fused, strike_attended, expiry_attended
                ], dim=0).mean(dim=0)

                weights = self.importance_weighting(attention_combined)
                weighted = attention_combined * weights

                pooled = torch.cat([
                    pool(weighted).flatten(1) for pool in self.multi_resolution_pools
                ], dim=1)

                return self.final_fusion(pooled)

        return LeanOptionChainEncoder(num_strikes, num_expirations, num_types, num_features)


    
    def _create_market_encoder(self, input_size: int) -> nn.Module:
        """Create encoder for market data with backwards compatibility for checkpoint loading"""
        # Create backwards compatible encoder that handles both 6 and 10 feature inputs
        class BackwardsCompatibleMarketEncoder(nn.Module):
            def __init__(self, expected_input_size):
                super().__init__()
                self.expected_input_size = expected_input_size
                
                # Create encoder expecting the current size (10 features)
                self.encoder = nn.Sequential(
                    nn.Linear(10, 32),  # Always expect 10 features internally
                    nn.LayerNorm(32),
                    nn.GELU(),
                    nn.Dropout(0.05),
                    nn.Linear(32, 64),
                    nn.LayerNorm(64),
                    GradientClipActivation()
                )
                
                # Adapter layer for 6->10 feature conversion if needed
                if expected_input_size == 6:
                    # Create adapter to convert 6 features to 10 features
                    self.feature_adapter = nn.Linear(6, 10)
                    # Initialize to preserve original 6 features and zero-pad new ones
                    with torch.no_grad():
                        # Identity mapping for first 6 features
                        self.feature_adapter.weight[:6, :] = torch.eye(6)
                        self.feature_adapter.weight[6:, :] = 0.0  # Zero weights for new features
                        self.feature_adapter.bias[:6] = 0.0
                        self.feature_adapter.bias[6:] = 0.0  # Zero bias for new features
                else:
                    self.feature_adapter = None
            
            def forward(self, x):
                if self.feature_adapter is not None and x.shape[-1] == 6:
                    # Convert 6 features to 10 features
                    x = self.feature_adapter(x)
                elif x.shape[-1] != 10:
                    # Pad or truncate to 10 features if needed
                    if x.shape[-1] < 10:
                        # Pad with zeros
                        padding = torch.zeros(*x.shape[:-1], 10 - x.shape[-1], device=x.device)
                        x = torch.cat([x, padding], dim=-1)
                    else:
                        # Truncate to 10 features
                        x = x[..., :10]
                
                return self.encoder(x)
        
        return BackwardsCompatibleMarketEncoder(input_size)
    def _apply_action_mask_efficient(self, logits_list, action_mask):
        """Apply action mask ONLY to first discrete action dimension using first 3 mask values."""
        
        if action_mask is None:
            return logits_list

        device = logits_list[0].device
        
        # Convert action_mask to tensor
        if isinstance(action_mask, np.ndarray):
            action_mask = torch.from_numpy(action_mask).to(dtype=torch.float32, device=device)
        elif isinstance(action_mask, torch.Tensor):
            action_mask = action_mask.to(dtype=torch.float32, device=device)
        else:
            raise TypeError("action_mask must be a NumPy array or PyTorch tensor")

        # FORCE FLATTEN - handle any shape issues
        action_mask = action_mask.flatten()
        
        masked_logits = []
        
        for i, logits in enumerate(logits_list):
            if i == 0:  # ONLY first discrete action dimension
                masked = logits.clone()
                num_actions = min(logits.shape[-1], 3, action_mask.shape[0])
                
                for j in range(num_actions):
                    if action_mask[j].item() == 0:
                        masked[..., j] = -1e4
                
                masked_logits.append(masked)
            else:
                # All other logits unchanged
                masked_logits.append(logits)

        return masked_logits
            
    def _create_portfolio_encoder(self, input_size: int) -> nn.Module:
        """Create encoder for portfolio data"""
        return nn.Sequential(
            nn.Linear(input_size, 32),
            nn.LayerNorm(32),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(32, 64),
            nn.LayerNorm(64),
            GradientClipActivation()
        )
    
    def _create_basic_encoder(self, input_size: int) -> nn.Module:
        """Create basic encoder for fallback cases"""
        return nn.Sequential(
            nn.Linear(input_size, 32),
            nn.LayerNorm(32),
            nn.GELU(),
            nn.Linear(32, 32)
        )
    
    def _create_positions_encoder(self, shape: tuple) -> nn.Module:
        """Create encoder for positions data with attention over positions - NO FLATTENING"""
        # shape is (max_positions, features) = (10, 8)
        max_positions, position_features = shape
        
        class PositionsEncoder(nn.Module):
            def __init__(self, position_features):
                super().__init__()
                # Per-position processing - preserves individual position structure
                self.per_position = nn.Linear(position_features, 32)  # 8 -> 32 features per position
                self.norm1 = nn.LayerNorm(32)
                self.activation1 = nn.GELU()
                self.dropout1 = nn.Dropout(0.1)
                
                # Position attention to focus on active positions while preserving position-level structure
                self.position_attention = nn.MultiheadAttention(
                    embed_dim=32,
                    num_heads=4,
                    dropout=0.1,
                    batch_first=True
                )
                
                # Position importance weighting (learn which positions matter most)
                self.position_importance = nn.Sequential(
                    nn.Linear(32, 16),
                    nn.GELU(),
                    nn.Linear(16, 1),
                    nn.Sigmoid()
                )
                
                # Final position-aware aggregation (weighted by importance, not simple mean)
                self.final_aggregation = nn.Sequential(
                    nn.Linear(32, 64),
                    nn.LayerNorm(64),
                    nn.GELU(),
                    nn.Linear(64, 128),
                    nn.LayerNorm(128),
                    GradientClipActivation()
                )
            
            def forward(self, x):
                # Input: (batch, 10, 8) - positions, features
                batch_size = x.shape[0]
                
                # Per-position processing - each position processed independently
                x = self.per_position(x)  # (batch, 10, 32)
                x = self.norm1(x)
                x = self.activation1(x)
                x = self.dropout1(x)
                
                # Position attention - positions can attend to each other
                attended_positions, attention_weights = self.position_attention(x, x, x)  # (batch, 10, 32)
                
                # Calculate position importance weights
                importance_weights = self.position_importance(attended_positions)  # (batch, 10, 1)
                
                # Weighted aggregation based on position importance (not simple averaging)
                # This preserves the contribution of important positions while reducing noise from empty positions
                weighted_positions = attended_positions * importance_weights  # (batch, 10, 32)
                
                # Sum weighted positions and normalize by total importance
                total_importance = torch.sum(importance_weights, dim=1, keepdim=True) + 1e-8  # (batch, 1, 1)
                
                # Fix dimension mismatch: sum across positions dimension, then divide by total importance
                position_sum = torch.sum(weighted_positions, dim=1)  # (batch, 32)
                total_importance_squeezed = total_importance.squeeze(-1)  # (batch, 1)
                aggregated = position_sum / total_importance_squeezed  # (batch, 32)
                
                # Final projection
                output = self.final_aggregation(aggregated)  # (batch, 128)
                
                return output
        
        return PositionsEncoder(position_features)
    
    def _create_environment_encoder(self, input_size: int) -> nn.Module:
        """Create encoder for environment state"""
        return nn.Sequential(
            nn.Linear(input_size, 16),
            nn.LayerNorm(16),
            nn.GELU(),
            nn.Linear(16, 32),
            nn.LayerNorm(32),
            GradientClipActivation()
        )
    
    def _create_generic_component_encoder(self, input_size: int) -> nn.Module:
        """Create generic encoder for other components"""
        return nn.Sequential(
            nn.Linear(input_size, 32),
            nn.LayerNorm(32),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(32, 64),
            nn.LayerNorm(64),
            GradientClipActivation()
        )
    def _create_component_fusion_network(self, component_feature_dims: dict, output_dim: int) -> nn.Module:
        """Create network to fuse component features into final representation"""
        total_input_dim = sum(component_feature_dims.values())  # This is 544
        
        class ComponentFusionNetwork(nn.Module):
            def __init__(self, total_input_dim, output_dim, residual_scale, component_feature_dims):
                super().__init__()
                self.output_dim = output_dim
                self.component_feature_dims = component_feature_dims
                self.expected_input_dim = total_input_dim
                
                self.fusion = nn.Sequential(
                    nn.Linear(total_input_dim, 512),  # 544 -> 512
                    LayerScale(512),
                    nn.LayerNorm(512),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    
                    nn.Linear(512, output_dim),  # 512 -> 544 (identity-like transformation)
                    nn.LayerNorm(output_dim),
                    GradientClipActivation()
                )
            
            def forward(self, x):
                # Check and fix dimension mismatch
                if x.shape[-1] != self.expected_input_dim:
                    worker_id = get_ray_worker_id()
                    log_extra = {'worker_id': worker_id}
                    worker_logger.error(f"Fusion input mismatch: expected {self.expected_input_dim}, got {x.shape[-1]}", extra=log_extra)
                    
                    # Emergency fix
                    if x.shape[-1] > self.expected_input_dim:
                        x = x[:, :self.expected_input_dim]
                    else:
                        padding = torch.zeros(x.shape[0], self.expected_input_dim - x.shape[-1], 
                                            device=x.device, dtype=x.dtype)
                        x = torch.cat([x, padding], dim=-1)
                
                result = self.fusion(x)
                return result
        
        return ComponentFusionNetwork(total_input_dim, output_dim, self.residual_scale, component_feature_dims)
    
    def _init_weights_manually(self):
        """Manually initialize weights to avoid recursion issues"""
        # Initialize encoder weights
        for module in [self.qf_encoder, self.qf_twin_encoder]:
            for layer in module.modules():
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight, gain=0.25)
                    if layer.bias is not None:
                        nn.init.constant_(layer.bias, 0.0)
                elif isinstance(layer, nn.LayerNorm):
                    nn.init.constant_(layer.bias, 0.0)
                    nn.init.constant_(layer.weight, 1.0)
        
        # Initialize Q-function weights
        for module in [self.qf, self.qf_twin]:
            for layer in module.modules():
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight, gain=0.25)
                    if layer.bias is not None:
                        nn.init.constant_(layer.bias, 0.0)
                elif isinstance(layer, nn.LayerNorm):
                    nn.init.constant_(layer.bias, 0.0)
                    nn.init.constant_(layer.weight, 1.0)
    
    def _register_gradient_hooks(self):
        """Register gradient clipping hooks on key parameters"""
        def clip_grad_hook(grad):
            return torch.clamp(grad, -self.gradient_clip_norm, self.gradient_clip_norm)
        
        # Apply to timing head
        for param in self.timing_head.parameters():
            if param.requires_grad:
                param.register_hook(clip_grad_hook)
        
        # Apply to parameter heads
        for head in self.parameter_heads.values():
            for param in head.parameters():
                if param.requires_grad:
                    param.register_hook(clip_grad_hook)
        
        # Apply to Q-functions
        for param in self.qf.parameters():
            if param.requires_grad:
                param.register_hook(clip_grad_hook)
        
        for param in self.qf_twin.parameters():
            if param.requires_grad:
                param.register_hook(clip_grad_hook)
    
    def _normalize_input(self, obs):
        """Robust input normalization for option chain observations"""
        # Handle potential NaN/inf values
        obs_clean = torch.where(torch.isnan(obs), torch.zeros_like(obs), obs)
        obs_clean = torch.where(torch.isinf(obs_clean), torch.sign(obs_clean) * 10.0, obs_clean)
        
        # Robust standardization using median and MAD (more stable than mean/std)
        obs_median = torch.median(obs_clean, dim=-1, keepdim=True)[0]
        obs_mad = torch.median(torch.abs(obs_clean - obs_median), dim=-1, keepdim=True)[0]
        
        # Normalize using median and MAD
        obs_norm = (obs_clean - obs_median) / (obs_mad + 1e-6)
        
        # Apply conservative clipping to prevent extreme values
        return torch.clamp(obs_norm, -3.0, 3.0)
    
    def _setup_temporal_lstm(self):
        """Setup state-of-the-art LSTM for temporal persistence learning"""
        worker_id = get_ray_worker_id()
        log_extra = {'worker_id': worker_id}
        
        # STATE-OF-THE-ART: Dynamic LSTM initialization (defer until we know feature size)
        self.lstm_hidden_size = 256  # Fixed optimal size
        self.lstm_num_layers = 2     # Proven optimal for RL tasks
        self.sequence_length = 64    # Extended window for long-term persistence patterns
        
        # These will be initialized on first use with correct feature size
        self.temporal_lstm = None
        self.temporal_attention = None
        self.lstm_layer_norm = None
        self.temporal_projector = None
        self.temporal_feature_size = None
        
        # Hidden state management for episodes
        self.lstm_hidden_states = {}  # Per-episode hidden states
        self.sequence_buffers = {}    # Per-episode sequence buffers
        self.use_temporal_lstm = True
        
        worker_logger.warning("[TEMPORAL-LSTM] LSTM will be initialized dynamically on first use", extra=log_extra)
    
    def _initialize_lstm_on_first_use(self, feature_size):
        """Initialize LSTM layers when we know the actual feature size"""
        if self.temporal_lstm is not None:
            return  # Already initialized
        
        self.temporal_feature_size = feature_size
        worker_id = get_ray_worker_id()
        log_extra = {'worker_id': worker_id}
        
        # Bidirectional LSTM with advanced features
        self.temporal_lstm = nn.LSTM(
            input_size=feature_size,
            hidden_size=self.lstm_hidden_size,
            num_layers=self.lstm_num_layers,
            batch_first=True,
            dropout=0.1,  # Light regularization
            bidirectional=False  # Causal for real-time trading
        )
        
        # Attention mechanism for sequence weighting
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=self.lstm_hidden_size,
            num_heads=8,  # Multi-head for rich patterns
            dropout=0.1,
            batch_first=True
        )
        
        # Layer normalization for training stability
        self.lstm_layer_norm = nn.LayerNorm(self.lstm_hidden_size)
        
        # Projection layers for feature enhancement
        self.temporal_projector = nn.Sequential(
            nn.Linear(self.lstm_hidden_size, self.lstm_hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.lstm_hidden_size, feature_size)  # Project back to feature_size
        )
        
        # Initialize parameters with Xavier/Orthogonal initialization
        for name, param in self.temporal_lstm.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param)
        
        # Move to correct device
        if hasattr(self, 'device') and self.device is not None:
            self.temporal_lstm = self.temporal_lstm.to(self.device)
            self.temporal_attention = self.temporal_attention.to(self.device)
            self.lstm_layer_norm = self.lstm_layer_norm.to(self.device)
            self.temporal_projector = self.temporal_projector.to(self.device)
        
        worker_logger.warning(f"[TEMPORAL-LSTM] Initialized with feature_size={feature_size}, hidden_size={self.lstm_hidden_size}", extra=log_extra)
        
    def _apply_temporal_processing(self, features, episode_id=None):
        """Apply LSTM temporal processing to encoded features"""
        if not getattr(self, 'use_temporal_lstm', False):
            return features
        
        # Generate episode_id if not provided
        if episode_id is None:
            episode_id = getattr(self, '_current_episode_id', 'default')
        
        # Initialize episode buffers if needed
        if episode_id not in self.sequence_buffers:
            self.sequence_buffers[episode_id] = []
            self.lstm_hidden_states[episode_id] = None
        
        # DYNAMIC INITIALIZATION: Initialize LSTM on first use with correct feature size
        if self.temporal_lstm is None:
            # Handle features tensor to get correct size
            if features.dim() > 2:
                flat_features = features.view(features.size(0), -1)
            else:
                flat_features = features
            
            feature_size = flat_features.size(-1)
            self._initialize_lstm_on_first_use(feature_size)
        
        # Add current features to sequence buffer (ensure proper 1D vector)
        if features.dim() > 2:
            # Flatten batch dimensions to handle multi-dimensional inputs
            flat_features = features.view(features.size(0), -1)
        else:
            flat_features = features
        
        # Extract single sample and ensure 1D vector
        if flat_features.size(0) > 1:
            single_sample = flat_features[0]  # Take first sample: [features]
        else:
            single_sample = flat_features.squeeze(0)  # Remove batch dim: [features]
        
        # Ensure it's 1D
        if single_sample.dim() == 0:
            single_sample = single_sample.unsqueeze(0)
        elif single_sample.dim() > 1:
            single_sample = single_sample.flatten()
        
        self.sequence_buffers[episode_id].append(single_sample.detach().clone())
        
        # Maintain sequence length
        if len(self.sequence_buffers[episode_id]) > self.sequence_length:
            self.sequence_buffers[episode_id] = self.sequence_buffers[episode_id][-self.sequence_length:]
        
        # Create sequence tensor
        if len(self.sequence_buffers[episode_id]) >= 2:  # Need at least 2 timesteps for temporal learning
            # Stack sequence: [seq_len, features] - each buffer element is 1D
            sequence = torch.stack(self.sequence_buffers[episode_id], dim=0)
            
            # Ensure sequence is 2D: [seq_len, features]
            if sequence.dim() > 2:
                sequence = sequence.view(sequence.size(0), -1)
            
            # Get current hidden state and ensure it matches sequence batch size
            current_hidden = self.lstm_hidden_states[episode_id]
            
            # Ensure hidden state batch size matches sequence
            if current_hidden is not None:
                expected_batch_size = 1  # Single sample processing
                if current_hidden[0].size(1) != expected_batch_size:
                    # Reset hidden state if batch size mismatch
                    current_hidden = None
            
            # Apply LSTM with proper input format [seq_len, batch_size=1, features]
            sequence_input = sequence.unsqueeze(1)  # Add batch dimension: [seq_len, 1, features]
            lstm_out, hidden_state = self.temporal_lstm(sequence_input, current_hidden)
            self.lstm_hidden_states[episode_id] = hidden_state
            
            # Apply attention to focus on relevant timesteps [seq_len, 1, hidden_size]
            attended_out, _ = self.temporal_attention(lstm_out, lstm_out, lstm_out)
            
            # Layer normalization
            normalized_out = self.lstm_layer_norm(attended_out)
            
            # Take the last timestep output and project back to feature space
            last_timestep = normalized_out[-1, 0, :]  # [hidden_size] - last seq step, batch=0
            temporal_features = self.temporal_projector(last_timestep)
            
            # Residual connection with original features
            enhanced_features = features + 0.1 * temporal_features  # Light temporal enhancement
            
            return enhanced_features
        else:
            # Not enough history yet, return original features
            return features
    
    def _reset_temporal_state(self, episode_id=None):
        """Reset temporal state for new episode"""
        if not getattr(self, 'use_temporal_lstm', False):
            return
        
        if episode_id is None:
            # Reset all states
            self.lstm_hidden_states.clear()
            self.sequence_buffers.clear()
        else:
            # Reset specific episode
            if episode_id in self.lstm_hidden_states:
                del self.lstm_hidden_states[episode_id]
            if episode_id in self.sequence_buffers:
                del self.sequence_buffers[episode_id]
    
    def _process_structured_obs_for_encoding(self, obs_dict):
        """Process structured observation dict while preserving structure for neural network input"""
        if isinstance(obs_dict, dict):
            # Use the HybridPolicyNetwork's processing method
            return self.pi._process_structured_obs(obs_dict)
        else:
            # Already processed or non-dict observation
            return obs_dict
    def _encode_obs(self, obs):
        """Encode observations with gradient stability - fixed cache keys"""
        # Use content-based cache key instead of id()
        if isinstance(obs, dict):
            # Create hash based on tensor contents, not object id
            cache_key = self._create_content_hash(obs)
        else:
            cache_key = hash(tuple(obs.flatten().detach().cpu().numpy())) if isinstance(obs, torch.Tensor) else str(obs)
        
        if cache_key not in self._encoder_cache:
            # Process and cache
            if isinstance(obs, dict) and hasattr(self, 'component_encoders') and self.component_encoders:
                
                obs_processed = self.pi._process_structured_obs_with_components(obs)
            elif isinstance(obs, dict):
                obs_processed = self._process_structured_obs_for_encoding(obs)
            else:
                obs_processed = obs
            
            # Store detached version to prevent gradient accumulation
            if isinstance(obs_processed, torch.Tensor):
                self._encoder_cache[cache_key] = obs_processed.detach()
            else:
                self._encoder_cache[cache_key] = obs_processed
        
        features = self._encoder_cache[cache_key]
        
        # Apply feature scaling
        if isinstance(features, torch.Tensor):
            scaled_features = features * self.feature_scale
            
            # STATE-OF-THE-ART: Apply temporal LSTM processing for persistence learning
            temporal_features = self._apply_temporal_processing(scaled_features)
            
            return temporal_features
        else:
            return features
    def _forward_shared(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """Shared forward pass with hybrid action handling - ATTENTION MASKED VERSION"""
        # CRITICAL: Ensure batch is on correct device
        batch = self._ensure_batch_device(batch)
        
        # CRITICAL: Create attention mask for padding before any processing
        attention_mask = self._create_attention_mask(batch)
        
        obs_data = batch["obs"]
        
        # Handle case where obs_data might be a tuple or other structure
        if isinstance(obs_data, tuple):
            obs_dict = obs_data[0] if len(obs_data) > 0 else {}
        else:
            obs_dict = obs_data
        
        # CRITICAL: Ensure obs_dict tensors are on correct device
        if isinstance(obs_dict, dict):
            obs_dict = self._ensure_batch_device(obs_dict)
        
        # ULTRA-FAST: Cache observation encoding WITH ATTENTION MASKING
        obs_key = f"masked_{id(obs_dict)}_{id(attention_mask)}"
        if obs_key not in self._encoder_cache:
            # Handle structured observations - preserve structure throughout
            if isinstance(obs_dict, dict):
                observations = obs_dict
                action_mask = obs_dict.get("action_mask")
            else:
                observations = obs_dict
                action_mask = None
            
            # CRITICAL: Encode observations WITH ATTENTION MASKING (no padding effects)
            policy_features = self._encode_obs_with_masking(observations, attention_mask)
            
            # CRITICAL: Ensure policy_features is on correct device
            policy_features = self._ensure_tensor_device(policy_features, "policy_features")
            if action_mask is not None:
                action_mask = self._ensure_tensor_device(action_mask, "action_mask")
            
            # Store in cache
            self._encoder_cache[obs_key] = (policy_features, action_mask)
        else:
            policy_features, action_mask = self._encoder_cache[obs_key]
            
            # CRITICAL: Ensure cached tensors are on correct device
            policy_features = self._ensure_tensor_device(policy_features, "cached_policy_features")
            if action_mask is not None:
                action_mask = self._ensure_tensor_device(action_mask, "cached_action_mask")
        
        # CRITICAL: Ensure model and tensor device consistency before trunk computation
        try:
            # Check if policy_trunk exists and get its device
            trunk_device = next(self.policy_trunk.parameters()).device
            
            # If devices don't match, move the tensor to the model's device
            if policy_features.device != trunk_device:
                policy_features = policy_features.to(trunk_device, non_blocking=True)
                
        except Exception as device_check_error:
            pass
        
        # CRITICAL: Apply trunk processing WITH ATTENTION MASKING
        trunk_features = self._apply_trunk_with_masking(policy_features, attention_mask)
        trunk_features = self._ensure_tensor_device(trunk_features, "trunk_features")
        
        # ULTRA-FAST: Batch compute all heads with attention masking
        head_outputs = self._compute_heads_with_attention_masking(trunk_features, attention_mask)
        
        for i, head_output in enumerate(head_outputs):
            # CRITICAL: Ensure head outputs are on correct device
            head_outputs[i] = self._ensure_tensor_device(head_output, f"head_output_{i}")
        
        # Apply action masking efficiently (separate from attention masking)
        if action_mask is not None:
            head_outputs = self._apply_action_mask_efficient(head_outputs, action_mask)
        
        # CRITICAL: Compute value function WITH ATTENTION MASKING
        vf_preds = self._compute_values_with_attention_masking(policy_features, attention_mask).squeeze(-1)
        vf_preds = self._ensure_tensor_device(vf_preds, "vf_preds")
        
        # Concatenate all head outputs for final action distribution
        action_dist_inputs = torch.cat(head_outputs, dim=-1)
        action_dist_inputs = self._ensure_tensor_device(action_dist_inputs, "action_dist_inputs")
        
        # DEBUG: Log shape information to understand the batch dimension issue
        worker_logger.warning(f"DEBUG: Before shape fix - action_dist_inputs.shape: {action_dist_inputs.shape}, "
                            f"head_outputs shapes: {[h.shape for h in head_outputs]}", 
                            extra={'worker_id': get_ray_worker_id()})
        
        # CRITICAL: Ensure correct batch dimensions for action distribution
        # If we have a sequence dimension (batch_size, seq_len, features), 
        # we need to handle it properly for single-step inference
        if len(action_dist_inputs.shape) > 2:
            # For sequences, take the last timestep (most recent)
            action_dist_inputs = action_dist_inputs[:, -1, :]
        elif len(action_dist_inputs.shape) == 2 and action_dist_inputs.shape[0] > 1:
            # Check if this is a spurious batch dimension from attention masking
            # If batch_size > 1 but we expect single actions, take the first element
            if hasattr(self, '_expecting_single_action') and self._expecting_single_action:
                action_dist_inputs = action_dist_inputs[0:1, :]
        
        # DEBUG: Log shape after fixes
        worker_logger.warning(f"DEBUG: After shape fix - action_dist_inputs.shape: {action_dist_inputs.shape}, "
                            f"vf_preds.shape: {vf_preds.shape}", 
                            extra={'worker_id': get_ray_worker_id()})
        
        # CRITICAL: Ensure value predictions have matching batch dimensions
        if len(vf_preds.shape) > 1 and vf_preds.shape[0] != action_dist_inputs.shape[0]:
            if len(vf_preds.shape) == 2:
                # Take last timestep for sequences
                vf_preds = vf_preds[:, -1]
            # Ensure batch dimension matches
            if vf_preds.shape[0] != action_dist_inputs.shape[0]:
                vf_preds = vf_preds[:action_dist_inputs.shape[0]]
        
        return {
            "action_dist_inputs": action_dist_inputs,
            "vf_preds": vf_preds,
        }
    def _setup_cuda_streams(self):
        """Setup CUDA streams for parallel processing"""
        # Check if CUDA is available AND if we're actually on a CUDA device
        if torch.cuda.is_available() and torch.cuda.current_device() >= 0:
            try:
                # Force CUDA initialization in this worker
                torch.cuda.init()
                
                # Check if any model parameters are on CUDA
                model_on_cuda = any(p.is_cuda for p in self.parameters())
                
                if model_on_cuda:
                    self.q_stream = torch.cuda.Stream()
                    self.q_twin_stream = torch.cuda.Stream()
                    self.encoder_stream = torch.cuda.Stream()
                    self.policy_stream = torch.cuda.Stream()
                    
                    worker_logger.warning(f"CUDA streams initialized on device {torch.cuda.current_device()}", 
                                        extra={'worker_id': get_ray_worker_id()})
                else:
                    worker_logger.warning("Model not on CUDA, streams set to None", 
                                        extra={'worker_id': get_ray_worker_id()})
                    self._set_streams_to_none()
            except Exception as e:
                worker_logger.warning(f"CUDA stream setup failed: {e}, falling back to CPU", 
                                    extra={'worker_id': get_ray_worker_id()})
                self._set_streams_to_none()
        else:
            worker_logger.warning("CUDA not available or no device set, streams set to None", 
                                extra={'worker_id': get_ray_worker_id()})
            self._set_streams_to_none()

    def _set_streams_to_none(self):
        """Helper to set all streams to None"""
        self.q_stream = None
        self.q_twin_stream = None
        self.encoder_stream = None
        self.policy_stream = None
    def _compute_q_values_stream(self, obs_batch, action_batch):
        """Q-value computation with parallel processing and memory-efficient caching"""
        
        # DEBUG: Check input tensor versions
        obs_version = getattr(obs_batch, '_version', 0)
        action_version = getattr(action_batch, '_version', 0)
        
        if obs_version > 20 or action_version > 20:
            worker_logger.warning(f"High version tensors in Q-compute: obs={obs_version}, action={action_version}")
            
            # FIX: Create fresh tensors if versions are too high
            if obs_version > 25:
                obs_batch = obs_batch.detach().requires_grad_(True)
                worker_logger.warning("Detached high-version obs tensor")
            if action_version > 25:
                action_batch = action_batch.detach().requires_grad_(True)
                worker_logger.warning("Detached high-version action tensor")
        
        # Periodic cache cleanup to prevent memory accumulation
        if len(self._q_input_cache) > 50:
            keys_to_remove = list(self._q_input_cache.keys())[:int(len(self._q_input_cache) * 0.75)]
            for key in keys_to_remove:
                self._q_input_cache.pop(key, None)
            
            worker_logger.warning(
                f"Q-input cache exceeded 50 entries, removed {len(keys_to_remove)} entries",
                extra={'worker_id': get_ray_worker_id()}
            )
        
        # Use tensor hashes for cache key
        cache_key = (
            obs_batch.shape,
            action_batch.shape,
            obs_batch.device,
            action_batch.device,
            hash(str(obs_batch.flatten()[:10].detach().cpu().numpy().tolist())),
            hash(str(action_batch.flatten()[:5].detach().cpu().numpy().tolist()))
        )
        
        # Check cache first
        if cache_key in self._q_input_cache:
            cached_result = self._q_input_cache[cache_key]
            if cached_result[0].requires_grad:
                worker_logger.warning("Cached tensor has gradients - creating fresh copy")
            return (cached_result[0].detach().requires_grad_(True), 
                    cached_result[1].detach().requires_grad_(True))
        
        # Use parallel Q-value computation
        q1_val, q2_val = self._compute_q_values_parallel_internal(obs_batch, action_batch)
        
        # DEBUG: Check output versions
        q1_version = getattr(q1_val, '_version', 0)
        q2_version = getattr(q2_val, '_version', 0)
        
        if q1_version > obs_version + 10 or q2_version > obs_version + 10:
            worker_logger.warning(f"Large version jump in Q-values: input={obs_version} -> q1={q1_version}, q2={q2_version}")
        
        # Store completely detached and cloned results
        result = (q1_val.detach().clone(), q2_val.detach().clone())
        self._q_input_cache[cache_key] = result
        
        return (q1_val, q2_val)

    # 2. ADD _compute_q_values_parallel_internal function
    def _compute_q_values_parallel_internal(self, obs_batch, action_batch):
        """Internal parallel Q-value computation"""
        # Ensure dtype consistency before concatenation
        obs_batch = obs_batch.float()
        action_batch = action_batch.float()
        
        # Ensure concatenation doesn't share memory
        obs_action_concat = torch.cat([obs_batch.clone(), action_batch.clone()], dim=-1)
        
        # Split batch across workers if batch is large enough
        batch_size = obs_action_concat.shape[0]
        
        if batch_size >= self.parallel_workers * 4:  # Only parallelize if batch is large enough
            return self._compute_q_values_batch_parallel(obs_action_concat)
        else:
            # Use standard computation for small batches
            return self._compute_q_values_standard(obs_action_concat)

    # 3. ADD _compute_q_values_batch_parallel function
    def _compute_q_values_batch_parallel(self, obs_action_concat):
        """Compute Q-values by splitting batch across CPU cores"""
        batch_size = obs_action_concat.shape[0]
        
        # CRITICAL: Ensure input is on correct device
        obs_action_concat = self._ensure_tensor_device(obs_action_concat, "obs_action_concat")
        
        # Split batch into chunks for parallel processing
        chunk_size = max(1, batch_size // self.parallel_workers)
        batch_chunks = [obs_action_concat[i:i + chunk_size] for i in range(0, batch_size, chunk_size)]
        
        def compute_q_chunk(chunk):
            """Compute Q-values for a chunk of the batch"""
            # CRITICAL: Ensure chunk is on correct device
            chunk = self._ensure_tensor_device(chunk, "q_chunk")
            
            # Q1 computation
            q1_features = self.qf_encoder(chunk)
            q1_val = self.qf(q1_features).squeeze(-1)
            
            # Q2 computation
            q2_features = self.qf_twin_encoder(chunk)
            q2_val = self.qf_twin(q2_features).squeeze(-1)
            
            return q1_val, q2_val
        
        # Execute chunks in parallel
        futures = []
        with ThreadPoolExecutor(max_workers=len(batch_chunks)) as executor:
            for chunk in batch_chunks:
                future = executor.submit(compute_q_chunk, chunk)
                futures.append(future)
            
            # Collect results
            q1_results = []
            q2_results = []
            for future in futures:
                q1_chunk, q2_chunk = future.result()
                q1_results.append(q1_chunk)
                q2_results.append(q2_chunk)
        
        # Concatenate results
        q1_val = torch.cat(q1_results, dim=0)
        q2_val = torch.cat(q2_results, dim=0)
        
        return q1_val, q2_val

    # 4. ADD _compute_q_values_standard function
    def _compute_q_values_standard(self, obs_action_concat):
        """Standard Q-value computation with optional CUDA streams"""
        if hasattr(self, 'q_stream') and self.q_stream is not None:
            with torch.cuda.stream(self.q_stream):
                q1_features = self.qf_encoder(obs_action_concat)
                
            with torch.cuda.stream(self.q_twin_stream):
                q2_features = self.qf_twin_encoder(obs_action_concat)
            
            torch.cuda.synchronize()
        else:
            q1_features = self.qf_encoder(obs_action_concat)
            q2_features = self.qf_twin_encoder(obs_action_concat)
        
        q1_val = self.qf(q1_features).squeeze(-1)
        q2_val = self.qf_twin(q2_features).squeeze(-1)
        
        return q1_val, q2_val

    # 5. ADD _compute_multiple_q_values_parallel function
    def _compute_multiple_q_values_parallel(self, obs_action_pairs):
        """Compute Q-values for multiple obs/action pairs in parallel"""
        if len(obs_action_pairs) == 1:
            obs_batch, action_batch = obs_action_pairs[0]
            return [self._compute_q_values_stream(obs_batch, action_batch)]
        
        def compute_single_q(obs_action_pair):
            obs_batch, action_batch = obs_action_pair
            return self._compute_q_values_stream(obs_batch, action_batch)
        
        # Execute multiple Q-computations in parallel
        futures = []
        max_workers = min(len(obs_action_pairs), self.parallel_workers)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for pair in obs_action_pairs:
                future = executor.submit(compute_single_q, pair)
                futures.append(future)
            
            # Collect results
            results = []
            for future in futures:
                result = future.result()
                results.append(result)
        
        return results

    def _create_attention_mask_from_padding(self, batch):
        """
        Create attention masks from padding metadata.
        Returns masks where 1=valid data, 0=padding (should be ignored).
        Returns None if no sequence dimension exists (single timestep sampling).
        """
        is_padded = batch.get('is_padded', None)
        valid_timesteps = batch.get('valid_timesteps', None)
        
        if is_padded is None or valid_timesteps is None:
            # No padding metadata - check if we have sequence data
            obs = batch.get('obs', {})
            if isinstance(obs, dict):
                # Get first tensor to infer shape
                first_tensor = next(iter(obs.values()))
                if isinstance(first_tensor, torch.Tensor):
                    if len(first_tensor.shape) < 2:
                        # No sequence dimension - single timestep sampling
                        return None
                    batch_size, seq_len = first_tensor.shape[:2]
                    if seq_len == 1:
                        # Single timestep - no masking needed
                        return None
                    # All data is valid (no padding) - create all-ones mask
                    device = first_tensor.device
                    return torch.ones(batch_size, seq_len, device=device)
                else:
                    return None
            else:
                if isinstance(obs, torch.Tensor):
                    if len(obs.shape) < 2:
                        # No sequence dimension - single timestep sampling
                        return None
                    batch_size, seq_len = obs.shape[:2]
                    if seq_len == 1:
                        # Single timestep - no masking needed
                        return None
                    # All data is valid (no padding) - create all-ones mask
                    device = obs.device
                    return torch.ones(batch_size, seq_len, device=device)
                else:
                    return None
        
        # Convert to tensors if needed
        device = next(iter(batch.values())).device if batch else torch.device('cpu')
        if not isinstance(is_padded, torch.Tensor):
            is_padded = torch.tensor(is_padded, device=device)
        if not isinstance(valid_timesteps, torch.Tensor):
            valid_timesteps = torch.tensor(valid_timesteps, device=device)
        
        batch_size = is_padded.shape[0]
        
        # Infer sequence length from batch data
        obs = batch.get('obs', {})
        if isinstance(obs, dict):
            first_tensor = next(iter(obs.values()))
            seq_len = first_tensor.shape[1] if isinstance(first_tensor, torch.Tensor) else 50
        else:
            seq_len = obs.shape[1] if isinstance(obs, torch.Tensor) else 50
        
        # Create attention mask: 1 for valid timesteps, 0 for padding
        attention_mask = torch.zeros(batch_size, seq_len, device=device)
        
        for i in range(batch_size):
            if is_padded[i]:
                # Episode is padded - mask only valid timesteps as 1
                valid_len = valid_timesteps[i].item()
                attention_mask[i, :valid_len] = 1.0
            else:
                # Episode not padded - all timesteps valid
                attention_mask[i, :] = 1.0
        
        return attention_mask

    def _apply_attention_mask_to_features(self, features, attention_mask):
        """
        Apply attention mask to features, ensuring padded positions don't affect valid ones.
        CRITICAL: This zeros out padded features AND their gradients.
        
        Handles variable tensor shapes gracefully to prevent dimension mismatches.
        """
        if attention_mask is None or features is None:
            return features
            
        # Handle different tensor shapes gracefully
        if features.ndim < 2:
            # Can't apply sequence-level masking to tensors with less than 2 dimensions
            return features
            
        # Get the actual shapes
        feature_shape = features.shape
        mask_shape = attention_mask.shape
        
        # Check if batch and sequence dimensions match
        if len(feature_shape) < 2 or len(mask_shape) < 2:
            return features
            
        if feature_shape[0] != mask_shape[0] or feature_shape[1] != mask_shape[1]:
            # Shape mismatch - return original features to avoid crash
            return features
            
        # Expand mask to match all feature dimensions
        # features: [batch_size, seq_len, dim1, dim2, ...]
        # attention_mask: [batch_size, seq_len] -> [batch_size, seq_len, 1, 1, ...]
        
        # Calculate how many dimensions to add
        num_extra_dims = len(feature_shape) - len(mask_shape)
        
        # Add singleton dimensions for broadcasting
        mask_expanded = attention_mask
        for _ in range(num_extra_dims):
            mask_expanded = mask_expanded.unsqueeze(-1)
        
        # Ensure both tensors are on the same device
        if features.device != mask_expanded.device:
            mask_expanded = mask_expanded.to(features.device)
        # Zero out padded positions - this prevents gradient flow from padding
        masked_features = features * mask_expanded
        return masked_features

    def _apply_attention_mask_to_logits(self, logits, attention_mask):
        """
        Apply attention mask to logits, setting padded positions to very negative values.
        This ensures padded positions don't contribute to probability distributions.
        """
        if attention_mask is None:
            return logits
            
        # For logits, we set padded positions to very negative values (-1e9)
        # instead of zero, so they have near-zero probability after softmax
        mask_expanded = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]
        
        # Where mask is 0 (padding), set logits to -1e9
        masked_logits = torch.where(
            mask_expanded.bool(),
            logits,
            torch.full_like(logits, -1e9)
        )
        
        return masked_logits

    def _masked_pool_features(self, features, attention_mask, pooling='mean'):
        """
        Pool features while ignoring padded positions.
        CRITICAL: Only valid timesteps contribute to pooling.
        """
        if attention_mask is None:
            # No masking needed
            if pooling == 'mean':
                return features.mean(dim=1)
            elif pooling == 'max':
                return features.max(dim=1)[0]
            elif pooling == 'sum':
                return features.sum(dim=1)
            else:
                return features[:, -1]  # Last timestep
        
        # Apply mask to features first
        masked_features = self._apply_attention_mask_to_features(features, attention_mask)
        
        if pooling == 'mean':
            # Compute mean only over valid timesteps
            valid_counts = attention_mask.sum(dim=1, keepdim=True)  # [batch_size, 1]
            valid_counts = torch.clamp(valid_counts, min=1)  # Avoid division by zero
            
            pooled = masked_features.sum(dim=1) / valid_counts  # [batch_size, feature_dim]
            return pooled
            
        elif pooling == 'max':
            # Set padded positions to very negative values before max pooling
            mask_expanded = attention_mask.unsqueeze(-1)
            masked_for_max = torch.where(
                mask_expanded.bool(),
                masked_features,
                torch.full_like(masked_features, -1e9)
            )
            return masked_for_max.max(dim=1)[0]
            
        elif pooling == 'sum':
            # Sum only valid positions (padding already zeroed)
            return masked_features.sum(dim=1)
            
        else:  # 'last' - get last valid timestep for each episode
            pooled = []
            for i in range(features.shape[0]):
                valid_mask = attention_mask[i]  # [seq_len]
                valid_indices = valid_mask.nonzero(as_tuple=True)[0]
                if len(valid_indices) > 0:
                    # Ensure the index is within bounds
                    last_valid_idx = valid_indices[-1]
                    seq_len = features.shape[1]
                    if last_valid_idx < seq_len:
                        selected_features = features[i, last_valid_idx]
                    else:
                        # Fallback to last available timestep
                        selected_features = features[i, seq_len - 1]
                else:
                    # No valid timesteps - use first timestep as fallback
                    selected_features = features[i, 0]
                
                # Ensure the selected features maintain their shape
                if selected_features.dim() == 0:
                    # If scalar, keep as scalar but ensure it's properly batched later
                    pooled.append(selected_features.unsqueeze(0))
                else:
                    pooled.append(selected_features)
            
            # Stack while preserving batch dimension
            result = torch.stack(pooled)
            # If result is 2D but should be 1D (e.g., value predictions), squeeze the last dim
            if result.dim() == 2 and result.shape[-1] == 1:
                result = result.squeeze(-1)
            return result

    def _filter_padding_from_batch(self, batch):
        """
        UPDATED: Instead of filtering, create attention masks for consistent tensor shapes.
        This ensures NO function learns from padded data while maintaining batch efficiency.
        
        Returns:
            batch: Original batch (unchanged for tensor shape consistency)
            attention_mask: Mask where 1=valid, 0=padding
        """
        attention_mask = self._create_attention_mask_from_padding(batch)
        
        # Log masking activity
        if attention_mask is not None:
            is_padded = batch.get('is_padded', None)
            if is_padded is not None:
                is_padded_tensor = torch.tensor(is_padded) if not isinstance(is_padded, torch.Tensor) else is_padded
                padded_count = is_padded_tensor.sum().item()
                total_count = is_padded_tensor.shape[0]
                
                if padded_count > 0:
                    valid_timesteps = batch.get('valid_timesteps', None)
                    if valid_timesteps is not None:
                        valid_timesteps_tensor = torch.tensor(valid_timesteps) if not isinstance(valid_timesteps, torch.Tensor) else valid_timesteps
                        avg_valid_len = valid_timesteps_tensor[is_padded_tensor].float().mean().item()
                        
                        worker_id = get_ray_worker_id()
                        worker_logger.info(
                            f"[ATTENTION-MASK] Masking {padded_count}/{total_count} padded episodes, "
                            f"avg valid length: {avg_valid_len:.1f} - PADDING MASKED OUT",
                            extra={'worker_id': worker_id}
                        )
        
        return batch, attention_mask

    def _reconstruct_padded_output(self, filtered_output, padding_info):
        """
        Reconstruct padded output from filtered results.
        This ensures output tensor shapes remain consistent.
        """
        if padding_info is None:
            return filtered_output
            
        # For now, return filtered output directly
        # The SAC algorithm will handle variable-length outputs
        return filtered_output

    def _forward_shared_with_masking(self, batch: Dict[str, Any], attention_mask) -> Dict[str, Any]:
        """Shared forward pass with attention masking - prevents learning from padding"""
        # CRITICAL: Ensure batch is on correct device
        batch = self._ensure_batch_device(batch)
        
        obs_data = batch["obs"]
        
        # Handle case where obs_data might be a tuple or other structure
        if isinstance(obs_data, tuple):
            obs_dict = obs_data[0] if len(obs_data) > 0 else {}
        else:
            obs_dict = obs_data
        
        # CRITICAL: Ensure obs_dict tensors are on correct device
        if isinstance(obs_dict, dict):
            obs_dict = self._ensure_batch_device(obs_dict)
        
        # ULTRA-FAST: Cache observation encoding with masking
        obs_key = id(obs_dict)
        if obs_key not in self._encoder_cache:
            # Handle structured observations - preserve structure throughout
            if isinstance(obs_dict, dict):
                observations = obs_dict
                action_mask = obs_dict.get("action_mask")
            else:
                observations = obs_dict
                action_mask = None
            
            # Encode observations with attention masking
            policy_features = self._encode_obs_with_masking(observations, attention_mask)
            
            # CRITICAL: Ensure policy_features is on correct device
            policy_features = self._ensure_tensor_device(policy_features, "policy_features")
            if action_mask is not None:
                action_mask = self._ensure_tensor_device(action_mask, "action_mask")
            
            # Store in cache
            self._encoder_cache[obs_key] = (policy_features, action_mask)
        else:
            policy_features, action_mask = self._encoder_cache[obs_key]
            
            # CRITICAL: Ensure cached tensors are on correct device
            policy_features = self._ensure_tensor_device(policy_features, "cached_policy_features")
            if action_mask is not None:
                action_mask = self._ensure_tensor_device(action_mask, "cached_action_mask")
        
        # CRITICAL: Ensure model and tensor device consistency before trunk computation
        try:
            # Check if policy_trunk exists and get its device
            trunk_device = next(self.policy_trunk.parameters()).device
            
            # If devices don't match, move the tensor to the model's device
            if policy_features.device != trunk_device:
                policy_features = policy_features.to(trunk_device, non_blocking=True)
                
        except Exception as device_check_error:
            pass
        
        trunk_features = self.policy_trunk(policy_features)
        trunk_features = self._ensure_tensor_device(trunk_features, "trunk_features")
        
        # CRITICAL: Apply attention mask to trunk features to prevent gradient flow from padding
        trunk_features = self._apply_attention_mask_to_features(trunk_features, attention_mask)
        
        # ULTRA-FAST: Batch compute all heads in parallel with masking
        head_outputs = self._compute_all_heads_batch_masked(trunk_features, attention_mask)
        
        for i, head_output in enumerate(head_outputs):
            # CRITICAL: Ensure head outputs are on correct device
            head_outputs[i] = self._ensure_tensor_device(head_output, f"head_output_{i}")
        
        # Apply action masking efficiently
        if action_mask is not None:
            head_outputs = self._apply_action_mask_efficient(head_outputs, action_mask)
        
        # Compute value function with attention masking
        vf_preds = self.value_head(policy_features)
        vf_preds = self._apply_attention_mask_to_features(vf_preds, attention_mask)
        vf_preds = self._masked_pool_features(vf_preds, attention_mask, 'last')
        
        # Carefully squeeze to maintain batch dimension
        if vf_preds.dim() > 1 and vf_preds.shape[-1] == 1:
            vf_preds = vf_preds.squeeze(-1)
        # Ensure we always have at least 1D tensor (batch dimension)
        if vf_preds.dim() == 0:
            vf_preds = vf_preds.unsqueeze(0)
            
        vf_preds = self._ensure_tensor_device(vf_preds, "vf_preds")
        
        # Concatenate all head outputs for final action distribution
        # Ensure all head outputs have proper batch dimensions
        processed_head_outputs = []
        for i, head_output in enumerate(head_outputs):
            if head_output.dim() == 0:
                # Scalar - add batch dimension
                head_output = head_output.unsqueeze(0)
            elif head_output.dim() == 1 and len(head_outputs) > 1:
                # Single dimension but multiple heads - might need feature dimension
                head_output = head_output.unsqueeze(-1)
            processed_head_outputs.append(head_output)
            
        action_dist_inputs = torch.cat(processed_head_outputs, dim=-1)
        action_dist_inputs = self._ensure_tensor_device(action_dist_inputs, "action_dist_inputs")
        
        # CRITICAL: Ensure correct batch dimensions for action distribution
        # Same fix as in _forward_shared to handle sequence/batch dimension issues
        if len(action_dist_inputs.shape) > 2:
            # For sequences, take the last timestep (most recent)
            action_dist_inputs = action_dist_inputs[:, -1, :]
        elif len(action_dist_inputs.shape) == 2 and action_dist_inputs.shape[0] > 1:
            # Check if this is a spurious batch dimension from attention masking
            if hasattr(self, '_expecting_single_action') and self._expecting_single_action:
                action_dist_inputs = action_dist_inputs[0:1, :]
        
        # Final safety check - ensure both outputs have batch dimensions
        if action_dist_inputs.dim() == 0:
            action_dist_inputs = action_dist_inputs.unsqueeze(0)
        if vf_preds.dim() == 0:
            vf_preds = vf_preds.unsqueeze(0)
        
        return {
            "action_dist_inputs": action_dist_inputs,
            "vf_preds": vf_preds,
        }

    def _encode_obs_with_masking(self, observations, attention_mask):
        """Encode observations with attention masking to prevent learning from padding"""
        # CRITICAL: Apply masking BEFORE any temporal processing to ensure 
        # LSTM never sees padded data
        
        # Get cache key for masked encoding
        cache_key = f"masked_obs_{self._create_content_hash(observations)}"
        
        if cache_key in self._encoder_cache:
            return self._encoder_cache[cache_key]
        
        # Process structured observations - preserve structure
        if isinstance(observations, dict) and hasattr(self, 'component_encoders') and self.component_encoders:
            # Apply attention masking to individual components BEFORE processing
            masked_components = {}
            for component_name, component_data in observations.items():
                if isinstance(component_data, torch.Tensor) and component_data.ndim >= 2:
                    # Apply attention mask to prevent padding influence
                    masked_components[component_name] = self._apply_attention_mask_to_features(
                        component_data, attention_mask
                    )
                else:
                    masked_components[component_name] = component_data
            
            # Process masked components (no padding effects in any encoder)
            features = self.pi._process_structured_obs_with_components(masked_components)
        elif isinstance(observations, dict):
            # Apply attention masking to dict observations
            masked_obs = {}
            for key, value in observations.items():
                if isinstance(value, torch.Tensor) and value.ndim >= 2:
                    masked_obs[key] = self._apply_attention_mask_to_features(value, attention_mask)
                else:
                    masked_obs[key] = value
            
            # Process masked observations
            features = self._process_structured_obs_for_encoding(masked_obs)
        else:
            # Apply attention masking to tensor observations
            masked_observations = self._apply_attention_mask_to_features(observations, attention_mask)
            features = masked_observations
        
        # Apply feature scaling to masked features
        if isinstance(features, torch.Tensor):
            scaled_features = features * self.feature_scale
            
            # CRITICAL: Apply temporal LSTM processing to MASKED features only
            # This ensures LSTM never sees padding and only learns from valid timesteps
            if hasattr(self, 'use_temporal_lstm') and self.use_temporal_lstm:
                # Process with attention mask to ensure LSTM ignores padding
                temporal_features = self._apply_temporal_processing_with_masking(
                    scaled_features, attention_mask
                )
            else:
                temporal_features = scaled_features
            
            # Cache and return
            self._encoder_cache[cache_key] = temporal_features
            return temporal_features
        else:
            self._encoder_cache[cache_key] = features
            return features

    def _apply_temporal_processing_with_masking(self, features, attention_mask, episode_id=None):
        """Apply LSTM temporal processing with attention masking - NO PADDING LEARNED"""
        if not getattr(self, 'use_temporal_lstm', False):
            return features
        
        # Generate episode_id if not provided
        if episode_id is None:
            episode_id = getattr(self, '_current_episode_id', 'default')
        
        # Initialize episode buffers if needed
        if episode_id not in self.sequence_buffers:
            self.sequence_buffers[episode_id] = []
            self.lstm_hidden_states[episode_id] = None
        
        # DYNAMIC INITIALIZATION: Initialize LSTM on first use with correct feature size
        if self.temporal_lstm is None:
            # Handle features tensor to get correct size
            if features.dim() > 2:
                flat_features = features.view(features.size(0), -1)
            else:
                flat_features = features
            
            feature_size = flat_features.size(-1)
            self._initialize_lstm_on_first_use(feature_size)
        
        # CRITICAL: Only add VALID (non-padded) timesteps to sequence buffer
        # Apply attention mask to filter out padding before LSTM processing
        if features.dim() > 2:
            flat_features = features.view(features.size(0), -1)
        else:
            flat_features = features
        
        # Apply attention mask to features BEFORE adding to sequence buffer
        if attention_mask is not None:
            # For sequence data with attention masking
            if attention_mask.dim() == 2:  # [batch_size, seq_len]
                batch_size, seq_len = attention_mask.shape
                if flat_features.shape[0] == batch_size * seq_len:
                    # Flattened sequence data
                    masked_features = flat_features * attention_mask.view(-1).unsqueeze(-1)
                    for i in range(flat_features.shape[0]):
                        if attention_mask.view(-1)[i].item() > 0.5:  # Valid timestep
                            single_sample = masked_features[i]
                            if single_sample.dim() == 0:
                                single_sample = single_sample.unsqueeze(0)
                            elif single_sample.dim() > 1:
                                single_sample = single_sample.flatten()
                            self.sequence_buffers[episode_id].append(single_sample.detach().clone())
                else:
                    # Batch data with sequence-level mask - use first valid timestep per batch
                    masked_features = flat_features * attention_mask[:, 0].unsqueeze(-1)  # Use first timestep mask
                    for i in range(batch_size):
                        if attention_mask[i, 0].item() > 0.5:  # Valid batch
                            single_sample = masked_features[i]
                            if single_sample.dim() == 0:
                                single_sample = single_sample.unsqueeze(0)
                            elif single_sample.dim() > 1:
                                single_sample = single_sample.flatten()
                            self.sequence_buffers[episode_id].append(single_sample.detach().clone())
            else:
                # Simple 1D mask
                masked_features = flat_features * attention_mask.unsqueeze(-1)
                for i in range(attention_mask.shape[0]):
                    if attention_mask[i].item() > 0.5:  # Valid timestep
                        single_sample = masked_features[i]
                        if single_sample.dim() == 0:
                            single_sample = single_sample.unsqueeze(0)
                        elif single_sample.dim() > 1:
                            single_sample = single_sample.flatten()
                        self.sequence_buffers[episode_id].append(single_sample.detach().clone())
        else:
            # No masking needed - add all timesteps
            for i in range(flat_features.shape[0]):
                single_sample = flat_features[i]
                if single_sample.dim() == 0:
                    single_sample = single_sample.unsqueeze(0)
                elif single_sample.dim() > 1:
                    single_sample = single_sample.flatten()
                
                self.sequence_buffers[episode_id].append(single_sample.detach().clone())
        
        # Maintain sequence length
        if len(self.sequence_buffers[episode_id]) > self.sequence_length:
            self.sequence_buffers[episode_id] = self.sequence_buffers[episode_id][-self.sequence_length:]
        
        # LSTM processing only on valid (non-padded) sequence
        if len(self.sequence_buffers[episode_id]) >= 2:
            # Stack sequence: [seq_len, features] - ALL VALID TIMESTEPS
            sequence = torch.stack(self.sequence_buffers[episode_id], dim=0)
            
            if sequence.dim() > 2:
                sequence = sequence.view(sequence.size(0), -1)
            
            current_hidden = self.lstm_hidden_states[episode_id]
            
            if current_hidden is not None:
                expected_batch_size = 1
                if current_hidden[0].size(1) != expected_batch_size:
                    current_hidden = None
            
            # Apply LSTM with proper input format [seq_len, batch_size=1, features]
            sequence_input = sequence.unsqueeze(1)  # Add batch dimension
            
            with torch.no_grad():
                lstm_out, new_hidden = self.temporal_lstm(sequence_input, current_hidden)
                self.lstm_hidden_states[episode_id] = new_hidden
            
            # Use the last LSTM output (most recent valid timestep)
            final_output = lstm_out[-1, 0, :]  # [hidden_size]
            
            # Apply layer norm and projection
            if hasattr(self, 'lstm_layer_norm') and self.lstm_layer_norm is not None:
                final_output = self.lstm_layer_norm(final_output)
            
            if hasattr(self, 'temporal_projector') and self.temporal_projector is not None:
                final_output = self.temporal_projector(final_output)
            
            # Expand back to match original batch size
            expanded_output = final_output.unsqueeze(0).expand(features.size(0), -1)
            return expanded_output
        else:
            # Not enough valid timesteps for LSTM - return masked features
            return features * attention_mask.unsqueeze(-1) if attention_mask is not None else features

    def _compute_all_heads_batch_masked(self, trunk_features, attention_mask):
        """Compute all policy heads in parallel with attention masking"""
        # First compute heads normally
        head_outputs = self._compute_all_heads_batch(trunk_features)
        
        # Then apply attention masking to logits
        masked_head_outputs = []
        for head_output in head_outputs:
            masked_output = self._apply_attention_mask_to_logits(head_output, attention_mask)
            masked_head_outputs.append(masked_output)
        
        return masked_head_outputs

    def _compute_q_values_with_attention_mask(self, observations, actions, attention_mask):
        """Compute Q-values with attention masking to prevent learning from padding"""
        # Apply attention mask to inputs first
        masked_obs = self._apply_attention_mask_to_features(observations, attention_mask)
        masked_acts = self._apply_attention_mask_to_features(actions, attention_mask)
        
        # Compute Q-values normally
        q1_val, q2_val = self._compute_q_values_stream(masked_obs, masked_acts)
        
        # Pool the Q-values using only valid timesteps
        q1_pooled = self._masked_pool_features(q1_val.unsqueeze(-1), attention_mask, 'last').squeeze(-1)
        q2_pooled = self._masked_pool_features(q2_val.unsqueeze(-1), attention_mask, 'last').squeeze(-1)
        
        return q1_pooled, q2_pooled

    def _compute_q_values_stream_masked(self, observations, actions, attention_mask):
        """Compute Q-values stream with attention masking"""
        return self._compute_q_values_with_attention_mask(observations, actions, attention_mask)

    def _compute_baselines_vectorized_masked(self, obs_dict, actions, structure_learns, attention_mask):
        """Compute counterfactual baselines with attention masking"""
        # Apply masking to inputs first
        if isinstance(obs_dict, dict):
            masked_obs_dict = {}
            for key, value in obs_dict.items():
                if isinstance(value, torch.Tensor):
                    masked_obs_dict[key] = self._apply_attention_mask_to_features(value, attention_mask)
                else:
                    masked_obs_dict[key] = value
        else:
            masked_obs_dict = self._apply_attention_mask_to_features(obs_dict, attention_mask)
        
        masked_actions = self._apply_attention_mask_to_features(actions, attention_mask)
        
        # Compute baselines normally with masked inputs and attention mask
        timing_baseline, structure_baseline = self._compute_baselines_vectorized(
            masked_obs_dict, masked_actions, structure_learns, attention_mask
        )
        
        return timing_baseline, structure_baseline

    def _compute_q_values_with_padding(self, observations, actions, is_padded, valid_timesteps, episode_length):
        """
        Compute Q-values with padding awareness for variable episode lengths.
        Only processes valid timesteps, not padding. No artificial scaling applied -
        Q-function naturally adapts to different trade lengths.
        
        Args:
            observations: Batch of observations [batch_size, seq_len, obs_dim]
            actions: Batch of actions [batch_size, seq_len, action_dim]
            is_padded: Tensor indicating if episodes are padded [batch_size]
            valid_timesteps: Number of valid timesteps per episode [batch_size]
            episode_length: Original episode length per episode [batch_size]
            
        Returns:
            q1_val, q2_val: Q-values computed only on valid data, no scaling
        """
        # If no padding metadata provided, compute normally
        if is_padded is None or valid_timesteps is None:
            return self._compute_q_values_stream(observations, actions)
            
        batch_size = observations.shape[0]
        device = observations.device
        
        # Convert to tensors if needed
        if not isinstance(is_padded, torch.Tensor):
            is_padded = torch.tensor(is_padded, device=device)
        if not isinstance(valid_timesteps, torch.Tensor):
            valid_timesteps = torch.tensor(valid_timesteps, device=device)
        
        # For non-padded episodes, compute normally
        non_padded_mask = ~is_padded.bool()
        
        if non_padded_mask.all():
            # No padding in this batch - compute normally
            return self._compute_q_values_stream(observations, actions)
        
        # Mixed batch: some padded, some not
        q1_vals = []
        q2_vals = []
        
        for i in range(batch_size):
            if non_padded_mask[i]:
                # Full episode - process normally
                obs_i = observations[i:i+1]  # Keep batch dimension
                act_i = actions[i:i+1]
                q1_i, q2_i = self._compute_q_values_stream(obs_i, act_i)
            else:
                # Padded episode - only process valid timesteps
                valid_len = valid_timesteps[i].item()
                
                # Extract only valid timesteps (no padding)
                obs_valid = observations[i:i+1, :valid_len]  # [1, valid_len, obs_dim]
                act_valid = actions[i:i+1, :valid_len]       # [1, valid_len, action_dim]
                
                # Compute Q-values on the actual trade length - let the Q-function
                # naturally adapt to different trade lengths without artificial scaling
                q1_i, q2_i = self._compute_q_values_stream(obs_valid, act_valid)
            
            q1_vals.append(q1_i)
            q2_vals.append(q2_i)
        
        # Concatenate results
        q1_val = torch.cat(q1_vals, dim=0)
        q2_val = torch.cat(q2_vals, dim=0)
        
        # Log padding statistics occasionally
        if hasattr(self, '_padding_log_counter'):
            self._padding_log_counter += 1
        else:
            self._padding_log_counter = 1
            
        if self._padding_log_counter % 100 == 0:  # Log every 100 calls
            padded_count = is_padded.sum().item()
            if padded_count > 0:
                avg_valid_len = valid_timesteps[is_padded].float().mean().item()
                worker_id = get_ray_worker_id()
                worker_logger.info(
                    f"[Q-PADDING] {padded_count}/{batch_size} episodes padded, "
                    f"avg valid length: {avg_valid_len:.1f} (no scaling applied)",
                    extra={'worker_id': worker_id}
                )
        
        return q1_val, q2_val
        """
        Compute Q-values with padding awareness for variable episode lengths.
        Only processes valid timesteps, not padding. No artificial scaling applied -
        Q-function naturally adapts to different trade lengths.
        
        Args:
            observations: Batch of observations [batch_size, seq_len, obs_dim]
            actions: Batch of actions [batch_size, seq_len, action_dim]
            is_padded: Tensor indicating if episodes are padded [batch_size]
            valid_timesteps: Number of valid timesteps per episode [batch_size]
            episode_length: Original episode length per episode [batch_size]
            
        Returns:
            q1_val, q2_val: Q-values computed only on valid data, no scaling
        """
        # If no padding metadata provided, compute normally
        if is_padded is None or valid_timesteps is None:
            return self._compute_q_values_stream(observations, actions)
            
        batch_size = observations.shape[0]
        device = observations.device
        
        # Convert to tensors if needed
        if not isinstance(is_padded, torch.Tensor):
            is_padded = torch.tensor(is_padded, device=device)
        if not isinstance(valid_timesteps, torch.Tensor):
            valid_timesteps = torch.tensor(valid_timesteps, device=device)
        
        # For non-padded episodes, compute normally
        non_padded_mask = ~is_padded.bool()
        
        if non_padded_mask.all():
            # No padding in this batch - compute normally
            return self._compute_q_values_stream(observations, actions)
        
        # Mixed batch: some padded, some not
        q1_vals = []
        q2_vals = []
        
        for i in range(batch_size):
            if non_padded_mask[i]:
                # Full episode - process normally
                obs_i = observations[i:i+1]  # Keep batch dimension
                act_i = actions[i:i+1]
                q1_i, q2_i = self._compute_q_values_stream(obs_i, act_i)
            else:
                # Padded episode - only process valid timesteps
                valid_len = valid_timesteps[i].item()
                
                # Extract only valid timesteps (no padding)
                obs_valid = observations[i:i+1, :valid_len]  # [1, valid_len, obs_dim]
                act_valid = actions[i:i+1, :valid_len]       # [1, valid_len, action_dim]
                
                # Compute Q-values on the actual trade length - let the Q-function
                # naturally adapt to different trade lengths without artificial scaling
                q1_i, q2_i = self._compute_q_values_stream(obs_valid, act_valid)
            
            q1_vals.append(q1_i)
            q2_vals.append(q2_i)
        
        # Concatenate results
        q1_val = torch.cat(q1_vals, dim=0)
        q2_val = torch.cat(q2_vals, dim=0)
        
        # Log padding statistics occasionally
        if hasattr(self, '_padding_log_counter'):
            self._padding_log_counter += 1
        else:
            self._padding_log_counter = 1
            
        if self._padding_log_counter % 100 == 0:  # Log every 100 calls
            padded_count = is_padded.sum().item()
            if padded_count > 0:
                avg_valid_len = valid_timesteps[is_padded].float().mean().item()
                worker_id = get_ray_worker_id()
                worker_logger.info(
                    f"[PADDING] {padded_count}/{batch_size} episodes padded, "
                    f"avg valid length: {avg_valid_len:.1f} (no scaling applied)",
                    extra={'worker_id': worker_id}
                )
        
        return q1_val, q2_val

    def _compute_timing_baseline_parallel(self, observations, actions, action_probs):
        """Parallel timing baseline computation across CPU cores"""
        device = observations.device
        batch_size = observations.shape[0]
        timing_baseline = torch.zeros(batch_size, dtype=torch.float32, device=device)
        
        # Split timing actions across workers
        timing_actions = list(range(self.timing_action_size))
        chunk_size = max(1, len(timing_actions) // self.parallel_workers)
        timing_chunks = [timing_actions[i:i + chunk_size] for i in range(0, len(timing_actions), chunk_size)]
        
        def process_timing_chunk(timing_chunk):
            chunk_baseline = torch.zeros(batch_size, dtype=torch.float32, device=device)
            for timing_action in timing_chunk:
                action_vector = actions.clone()
                action_vector[:, 0] = timing_action
                action_concat = _batch_one_hot_encode_fast_jit(action_vector, self.action_component_sizes)
                q1_val, _ = self._compute_q_values_stream(observations, action_concat)
                chunk_baseline = chunk_baseline + action_probs[0][:, timing_action] * q1_val
            return chunk_baseline
        
        # Execute in parallel
        with ThreadPoolExecutor(max_workers=len(timing_chunks)) as executor:
            futures = [executor.submit(process_timing_chunk, chunk) for chunk in timing_chunks]
            results = [future.result() for future in futures]
        
        return sum(results)
    def _compute_action_probs_batch(self, trunk_features):
        """Compute all action probabilities in batch - ULTRA-FAST"""
        action_probs = []
        
        # Timing head
        timing_logits = self.timing_head(trunk_features)
        timing_probs = F.softmax(timing_logits, dim=-1)
        action_probs.append(timing_probs)
        
        # Parameter heads - batch compute
        for param_config in self.parameter_configs:
            param_name = param_config['name']
            param_logits = self.parameter_heads[param_name](trunk_features)
            if param_config['type'] == 'discrete':
                param_probs = F.softmax(param_logits, dim=-1)
            else:
                # For continuous parameters, create dummy probability tensor
                param_probs = torch.ones(param_logits.shape[0], 1, device=param_logits.device)
            action_probs.append(param_probs)
        
        return action_probs


    def _compute_all_heads_batch(self, trunk_features):
        """Compute all policy heads in parallel - ULTRA-FAST"""
        # ULTRA-FAST: Pre-allocate output list
        head_outputs = [None] * (1 + len(self.parameter_configs))
        
        # Conditional learning check
        if self.enable_conditional_learning:
            # Get position state once
            has_open_position = hasattr(self, '_current_position_state') and self._current_position_state
            
            if has_open_position:
                # Timing gets full gradients
                head_outputs[0] = self.timing_head(trunk_features)
                
                # Parameters get scaled gradients
                for i, param_config in enumerate(self.parameter_configs):
                    param_name = param_config['name']
                    param_output = self.parameter_heads[param_name](trunk_features)
                    param_output = self._apply_conditional_gradient_scaling(
                        param_output, self.parameter_detach_strength
                    )
                    head_outputs[i + 1] = param_output
            else:
                # Both policies learn normally
                head_outputs[0] = self.timing_head(trunk_features)
                
                for i, param_config in enumerate(self.parameter_configs):
                    param_name = param_config['name']
                    head_outputs[i + 1] = self.parameter_heads[param_name](trunk_features)
        else:
            # No conditional learning
            head_outputs[0] = self.timing_head(trunk_features)
            
            for i, param_config in enumerate(self.parameter_configs):
                param_name = param_config['name']
                head_outputs[i + 1] = self.parameter_heads[param_name](trunk_features)
        
        return head_outputs

    @override(ValueFunctionAPI)
    def compute_values(self, batch, **kwargs):
        """Compute value function predictions with gradient stability - handles structured observations"""
        # CRITICAL: Create attention masks instead of filtering for value computation
        batch, attention_mask = self._filter_padding_from_batch(batch)
        
        obs_dict = batch["obs"]
        
        # Handle structured observations without flattening using attention masking
        observations = obs_dict
        
        # Use attention masking for encoding to prevent padding effects on LSTM/temporal processing
        feats = self._encode_obs_with_masking(observations, attention_mask)
        values = self.value_head(feats).squeeze(-1)
        
        # Apply attention mask to values to ensure padding positions don't contribute
        if attention_mask is not None:
            # Ensure values shape matches attention_mask for proper masking
            if len(values.shape) == 2 and len(attention_mask.shape) == 2:
                # For sequence-level masking, pool over valid timesteps only
                mask_expanded = attention_mask.float()  # Convert to float for masking
                valid_lengths = mask_expanded.sum(dim=1, keepdim=True).clamp(min=1)
                values = (values * mask_expanded).sum(dim=1, keepdim=True) / valid_lengths
                values = values.squeeze(-1)
            elif len(values.shape) == 1:
                # For batch-level values, no additional masking needed
                pass
        
        return torch.clamp(values, min=-100.0, max=100.0)
    
    def _forward_inference(self, batch: Dict[str, Any], **kwargs):
        # Lightweight logging for sampling/inference activity
        self._sampling_count += 1
        if self._sampling_count % self._log_every_n == 0:
            worker_id = get_ray_worker_id()
            worker_logger.info(f"[SAMPLING] Agent sampled {self._sampling_count} times for inference", 
                              extra={'worker_id': worker_id})
        
        # CRITICAL: Create attention masks for sequence data, use old method for single timesteps
        batch, attention_mask = self._filter_padding_from_batch(batch)
        
        # CRITICAL: Ensure batch is on correct device for inference
        batch = self._ensure_batch_device(batch)
        
        # Use attention masking for sequence data, fall back to old method for single timesteps
        if attention_mask is not None:
            return self._forward_shared_with_masking(batch, attention_mask)
        else:
            return self._forward_shared(batch)

    def _forward_exploration(self, batch: Dict[str, Any], **kwargs):
        # Lightweight logging for exploration/sampling activity  
        self._sampling_count += 1
        if self._sampling_count % self._log_every_n == 0:
            worker_id = get_ray_worker_id()
            worker_logger.info(f"[SAMPLING] Agent sampled {self._sampling_count} times for exploration", 
                              extra={'worker_id': worker_id})
        
        # DEBUG: Log batch shapes to understand the action dimension issue
        obs_data = batch.get("obs", {})
        if isinstance(obs_data, dict):
            for key, value in obs_data.items():
                if isinstance(value, torch.Tensor):
                    worker_logger.warning(f"DEBUG: Batch obs[{key}] shape: {value.shape}", 
                                        extra={'worker_id': get_ray_worker_id()})
        elif isinstance(obs_data, torch.Tensor):
            worker_logger.warning(f"DEBUG: Batch obs shape: {obs_data.shape}", 
                                extra={'worker_id': get_ray_worker_id()})
        
        # CRITICAL: Create attention masks for sequence data, use old method for single timesteps
        batch, attention_mask = self._filter_padding_from_batch(batch)
        
        # CRITICAL: Ensure batch is on correct device for exploration  
        batch = self._ensure_batch_device(batch)
        
        # Use attention masking for sequence data, fall back to old method for single timesteps
        if attention_mask is not None:
            return self._forward_shared_with_masking(batch, attention_mask)
        else:
            return self._forward_shared(batch)
    
    def log_learning_sampling_summary(self):
        """Log summary of learning vs sampling activity"""
        if self._learning_count > 0 or self._sampling_count > 0:
            worker_id = get_ray_worker_id()
            total_calls = self._learning_count + self._sampling_count
            learning_ratio = self._learning_count / total_calls if total_calls > 0 else 0
            worker_logger.info(f"[ACTIVITY] Learning: {self._learning_count}, Sampling: {self._sampling_count}, "
                              f"Learning ratio: {learning_ratio:.2f}", 
                              extra={'worker_id': worker_id})
    
    def clear_caches(self):
        """Clear all internal caches to free memory"""
        self._encoder_cache.clear()
        self._component_batch_cache.clear()
        self._q_input_cache.clear()
        self._attention_cache.clear()
        # Don't clear tensor pool as it's for reuse

    def _ensure_device_efficient(self, tensors, target_device):
        """Batch device transfers - ULTRA-FAST"""
        return [t.to(target_device, non_blocking=True) if t.device != target_device else t for t in tensors]
    def _compute_baselines_vectorized(self, obs_dict, actions, structure_learns, attention_mask=None):
        """Compute counterfactual baselines with parallel processing"""
        observations = self._process_structured_obs_for_encoding(obs_dict)
        
        if not isinstance(observations, torch.Tensor):
            observations = torch.as_tensor(observations, dtype=torch.float32)
        
        device = observations.device
        batch_size = actions.shape[0]
        
        # Get policy features with attention masking if available
        if attention_mask is not None:
            policy_features = self._encode_obs_with_masking(obs_dict, attention_mask)
        else:
            # Create dummy attention mask if not provided (for backward compatibility)
            dummy_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=device)
            policy_features = self._encode_obs_with_masking(obs_dict, dummy_mask)
        
        # CRITICAL: Ensure model and tensor device consistency before trunk computation
        try:
            # Check if policy_trunk exists and get its device
            trunk_device = next(self.policy_trunk.parameters()).device
            worker_logger.warning(f"[DEVICE BASELINE] policy_trunk on device: {trunk_device}, policy_features on device: {policy_features.device}", extra={'worker_id': get_ray_worker_id()})
            
            # If devices don't match, move the tensor to the model's device
            if policy_features.device != trunk_device:
                worker_logger.warning(f"[DEVICE BASELINE] Device mismatch detected! Moving policy_features from {policy_features.device} to {trunk_device}", extra={'worker_id': get_ray_worker_id()})
                policy_features = policy_features.to(trunk_device, non_blocking=True)
                
        except Exception as device_check_error:
            worker_logger.error(f"[DEVICE BASELINE] Error checking device consistency: {device_check_error}", extra={'worker_id': get_ray_worker_id()})
        
        trunk_features = self.policy_trunk(policy_features)
        action_probs = self._compute_action_probs_batch(trunk_features)
        
        # Parallel timing baseline
        timing_baseline = self._compute_timing_baseline_parallel(observations, actions, action_probs)
        
        # Structure baseline for OPEN actions
        structure_baseline = torch.zeros(batch_size, dtype=torch.float32, device=device)
        if structure_learns.any():
            open_indices = torch.nonzero(structure_learns, as_tuple=True)[0]
            if open_indices.numel() > 0:
                structure_baseline_open = self._compute_structure_baseline_parallel(
                    observations[open_indices], actions[open_indices], 
                    [probs[open_indices] for probs in action_probs]
                )
                structure_baseline = structure_baseline.index_copy(0, open_indices, structure_baseline_open)
        
        return timing_baseline, structure_baseline
    def _compute_structure_baseline_parallel(self, observations, actions, action_probs):
        """Parallel structure baseline computation"""
        device = observations.device
        batch_size = observations.shape[0]
        structure_baseline = torch.zeros(batch_size, dtype=torch.float32, device=device)
        
        def process_structure_head(head_idx):
            component_baseline = torch.zeros(batch_size, dtype=torch.float32, device=device)
            action_size = self.action_component_sizes[head_idx]
            
            for action_val in range(action_size):
                action_vector = actions.clone()
                action_vector[:, head_idx] = action_val
                action_concat = _batch_one_hot_encode_fast_jit(action_vector, self.action_component_sizes)
                q1_val, _ = self._compute_q_values_stream(observations, action_concat)
                component_baseline = component_baseline + action_probs[head_idx][:, action_val] * q1_val
            
            return component_baseline
        
        # Process structure heads in parallel
        structure_head_indices = list(range(1, len(self.action_component_sizes)))
        with ThreadPoolExecutor(max_workers=min(len(structure_head_indices), self.parallel_workers)) as executor:
            futures = [executor.submit(process_structure_head, head_idx) for head_idx in structure_head_indices]
            results = [future.result() for future in futures]
        
        if results:
            structure_baseline = sum(results) / len(results)
        
        return structure_baseline    
    def _forward_train_core(self, batch: Dict[str, Any], attention_mask=None, **kwargs):
        """Training forward pass with attention masking - PADDING-AWARE VERSION"""
        
        # CRITICAL: Use provided attention mask or create one if not provided
        # This maintains tensor shape consistency while preventing learning from padding
        if attention_mask is None:
            batch, attention_mask = self._filter_padding_from_batch(batch)
        
        # OPTIMIZATION: Check tensor versions for debugging - EXACT SAME LOGIC
        if self.debug_tensor_versions_comprehensive():
            worker_logger.error("HIGH VERSIONS DETECTED - CLEARING ALL CACHES", 
                               extra={'worker_id': get_ray_worker_id()})
            self.clear_episode_caches()
        
        # Use optimized shared forward pass with attention masking
        res = self._forward_shared_with_masking(batch, attention_mask)
        
        obs_data = batch["obs"]
        
        # Handle case where obs_data might be a tuple or other structure - EXACT SAME
        if isinstance(obs_data, tuple):
            obs_dict = obs_data[0] if len(obs_data) > 0 else {}
        else:
            obs_dict = obs_data

        # Handle structured observations - preserve structure for gradient computation - EXACT SAME
        if isinstance(obs_dict, dict):
            observations = obs_dict
        else:
            observations = obs_dict

        # OPTIMIZATION: Process observations for Q-function input while preserving gradients
        obs_cache_key = f"train_{self._create_content_hash(observations)}"
        if obs_cache_key not in self._encoder_cache:
            if isinstance(observations, dict) and hasattr(self, 'component_encoders') and self.component_encoders:
                observations_processed = self.pi._process_structured_obs_with_components(observations)
                observations_processed = observations_processed.detach()
                observations_processed.requires_grad_(True)
                # CRITICAL: Apply attention mask to prevent gradient flow from padding
                observations_processed = self._apply_attention_mask_to_features(observations_processed, attention_mask)
            elif isinstance(observations, dict):
                observations_processed = self._process_structured_obs_for_encoding(observations)
                observations_processed = observations_processed.detach()
                observations_processed.requires_grad_(True)
                # CRITICAL: Apply attention mask to prevent gradient flow from padding
                observations_processed = self._apply_attention_mask_to_features(observations_processed, attention_mask)
            else:
                observations = observations.detach()
                observations.requires_grad_(True)
                observations_processed = observations
                # CRITICAL: Apply attention mask to prevent gradient flow from padding
                observations_processed = self._apply_attention_mask_to_features(observations_processed, attention_mask)
            
            self._encoder_cache[obs_cache_key] = observations_processed
        else:
            observations_processed = self._encoder_cache[obs_cache_key]

        actions = batch["actions"]

        # Extract timing and structure actions - EXACT SAME
        timing_actions = actions[:, 0]  # OPEN=0, HOLD=1, CLOSE=2
        structure_actions = actions[:, 1:]

        # Create masks for which policies should learn - EXACT SAME
        timing_learns = torch.ones_like(timing_actions, dtype=torch.bool)  # Always learns
        structure_learns = (timing_actions == 0)  # Only learn when OPEN action

        # OPTIMIZATION: Use JIT compiled one-hot encoding
        self.debug_action_space(actions)
        # Ensure actions are float32 before encoding
        actions = actions.float()
        act_concat = _batch_one_hot_encode_fast_jit(actions, self.action_component_sizes)

        # CRITICAL: Apply attention mask to actions to prevent learning from padded actions
        act_concat = self._apply_attention_mask_to_features(act_concat, attention_mask)

        # Q-values for taken actions using batch computation with attention masking
        # Pass attention mask to Q-function for proper masking
        is_padded = batch.get('is_padded', None)
        valid_timesteps = batch.get('valid_timesteps', None)
        episode_length = batch.get('episode_length', None)
        
        # Ensure dtype consistency before Q-value computation
        observations_processed = observations_processed.float()
        act_concat = act_concat.float()
        
        # Compute Q-values with attention masking (no padding learning)
        q1_val, q2_val = self._compute_q_values_with_attention_mask(
            observations_processed, act_concat, attention_mask
        )

        # Counterfactual baselines using vectorized computation with attention masking
        timing_baseline, structure_baseline = self._compute_baselines_vectorized_masked(
            obs_dict, actions, structure_learns, attention_mask
        )

        timing_advantage = q1_val - timing_baseline
        structure_advantage = torch.where(
            structure_learns,
            q1_val - structure_baseline,
            torch.zeros_like(q1_val).detach()
        )

        # Resample actions from current policy using cached features with attention masking
        policy_cache_key = f"policy_{self._create_content_hash(observations)}"
        if policy_cache_key not in self._encoder_cache:
            policy_feats = self._encode_obs_with_masking(observations, attention_mask)
            trunk_feats = self.policy_trunk(policy_feats)
            # CRITICAL: Mask trunk features to prevent gradient flow from padding
            trunk_feats = self._apply_attention_mask_to_features(trunk_feats, attention_mask)
            self._encoder_cache[policy_cache_key] = trunk_feats
        else:
            trunk_feats = self._encoder_cache[policy_cache_key]
        
        # Batch create all logits with attention masking
        all_logits = self._compute_all_heads_batch_masked(trunk_feats, attention_mask)
        
        # Apply masking efficiently before distribution creation - EXACT SAME
        action_mask = obs_dict.get("action_mask") if isinstance(obs_dict, dict) else None
        if action_mask is not None:
            all_logits = self._apply_action_mask_efficient(all_logits, action_mask)

        # Create distribution from properly masked logits - EXACT SAME
        policy_dist = self.get_train_action_dist_cls().from_logits(
            torch.cat(all_logits, dim=-1), input_lens=self.action_component_sizes
        )
        resampled = policy_dist.sample()
        logp_res = policy_dist.logp(resampled)

        # OPTIMIZATION: Use JIT compiled one-hot encoding
        res_concat = _batch_one_hot_encode_fast_jit(resampled, self.action_component_sizes)
        
        # CRITICAL: Apply attention mask to resampled actions
        res_concat = self._apply_attention_mask_to_features(res_concat, attention_mask)
        
        # Use batch Q-value computation with attention masking
        q_curr, q_curr_twin = self._compute_q_values_stream_masked(observations_processed, res_concat, attention_mask)

        # Next state target Q-values with attention masking
        q_target_next = torch.zeros_like(q1_val)
        q_target_next_twin = torch.zeros_like(q2_val)
        logp_next_res = torch.zeros_like(logp_res)

        if "obs_next" in batch:
            next_obs_data = batch["obs_next"]
            
            # Handle case where next_obs_data might be a tuple or other structure - EXACT SAME
            if isinstance(next_obs_data, tuple):
                next_obs_dict = next_obs_data[0] if len(next_obs_data) > 0 else {}
            else:
                next_obs_dict = next_obs_data
            
            # Handle structured next observations - preserve structure - EXACT SAME
            if isinstance(next_obs_dict, dict):
                next_obs = next_obs_dict
            else:
                next_obs = next_obs_dict
            
            # Cache next observation processing with attention masking
            next_obs_key = f"next_{self._create_content_hash(next_obs)}"
            if next_obs_key not in self._encoder_cache:
                next_feats = self._encode_obs_with_masking(next_obs, attention_mask)
                next_trunk_feats = self.policy_trunk(next_feats)
                # CRITICAL: Mask next trunk features
                next_trunk_feats = self._apply_attention_mask_to_features(next_trunk_feats, attention_mask)
                self._encoder_cache[next_obs_key] = next_trunk_feats
            else:
                next_trunk_feats = self._encoder_cache[next_obs_key]
            
            # Batch create next logits with attention masking
            next_logits = self._compute_all_heads_batch_masked(next_trunk_feats, attention_mask)
            
            next_mask = next_obs_dict.get("action_mask") if isinstance(next_obs_dict, dict) else None
            if next_mask is not None:
                next_logits = self._apply_action_mask_efficient(next_logits, next_mask)

            next_dist = self.get_train_action_dist_cls().from_logits(
                torch.cat(next_logits, dim=-1), input_lens=self.action_component_sizes
            )
            next_act = next_dist.sample()
            logp_next_res = next_dist.logp(next_act)

            # OPTIMIZATION: Use JIT compiled one-hot encoding
            next_concat = _batch_one_hot_encode_fast_jit(next_act, self.action_component_sizes)
            
            # CRITICAL: Apply attention mask to next actions
            next_concat = self._apply_attention_mask_to_features(next_concat, attention_mask)
            
            # Process next observations for Q-value computation with masking
            next_obs_processed = self._process_structured_obs_for_encoding(next_obs) if isinstance(next_obs, dict) else next_obs
            next_obs_processed = self._apply_attention_mask_to_features(next_obs_processed, attention_mask)
            next_q_in = torch.cat([next_obs_processed, next_concat], dim=-1)

            with torch.no_grad():
                next_q_feat1 = self.qf_target_encoder(next_q_in)
                next_q_feat2 = self.qf_twin_target_encoder(next_q_in)
                # CRITICAL: Mask Q features before final prediction
                next_q_feat1 = self._apply_attention_mask_to_features(next_q_feat1, attention_mask)
                next_q_feat2 = self._apply_attention_mask_to_features(next_q_feat2, attention_mask)
                
                # Pool masked features for final Q-values
                q_target_next = self._masked_pool_features(self.qf_target(next_q_feat1), attention_mask, 'last')
                q_target_next_twin = self._masked_pool_features(self.qf_twin_target(next_q_feat2), attention_mask, 'last')

        res.update({
            "qf_preds": self._stabilize_values(q1_val, "qf_preds"),
            "qf_twin_preds": self._stabilize_values(q2_val, "qf_twin_preds"),
            "q_curr": self._stabilize_values(q_curr, "q_curr"),
            "q_curr_twin": self._stabilize_values(q_curr_twin, "q_curr_twin"),
            "q_target_next": q_target_next.detach(),
            "q_target_next_twin": q_target_next_twin.detach(),
            "logp_resampled": self._stabilize_values(logp_res, "logp_resampled"),
            "logp_next_resampled": self._stabilize_values(logp_next_res, "logp_next_resampled"),
            "timing_advantage": self._stabilize_values(timing_advantage, "timing_advantage"),
            "structure_advantage": self._stabilize_values(structure_advantage, "structure_advantage"),
            "timing_learns": timing_learns,
            "structure_learns": structure_learns,
        })

        return res

    def debug_tensor_versions_comprehensive(self):
        """Comprehensive tensor version debugging"""
        high_version_items = []
        
        # Check all parameters - increased threshold to reduce false positives during normal training
        for name, param in self.named_parameters():
            if hasattr(param, '_version') and param._version > 50:  # Increased from 10 to 50
                high_version_items.append(f"PARAM {name}: {param._version}")
        
        # Check all buffers  
        for name, buffer in self.named_buffers():
            if buffer is not None and hasattr(buffer, '_version') and buffer._version > 50:  # Increased threshold
                high_version_items.append(f"BUFFER {name}: {buffer._version}")
        
        # Check cached tensors
        for cache_name, cache in [('encoder', self._encoder_cache), 
                                ('q_input', self._q_input_cache),
                                ('attention', self._attention_cache)]:
            for key, value in cache.items():
                if isinstance(value, torch.Tensor) and hasattr(value, '_version') and value._version > 50:  # Increased threshold
                    high_version_items.append(f"CACHE {cache_name}[{key}]: {value._version}")
                elif isinstance(value, (tuple, list)):
                    for i, item in enumerate(value):
                        if isinstance(item, torch.Tensor) and hasattr(item, '_version') and item._version > 50:  # Increased threshold
                            high_version_items.append(f"CACHE {cache_name}[{key}][{i}]: {item._version}")
        
        if high_version_items:
            worker_logger.error(f"HIGH VERSION TENSORS DETECTED:\n" + "\n".join(high_version_items),
                               extra={'worker_id': get_ray_worker_id()})
            return True
        return False

    def _stabilize_values(self, tensor, name=""):
        """Stabilize tensor values to prevent numerical issues while preserving gradients"""
        if torch.isnan(tensor).any() or torch.isinf(tensor).any():
            worker_logger.warning(f"Found NaN/Inf in {name}, replacing with zeros", extra={'worker_id': get_ray_worker_id()})
            # Preserve gradient tracking when replacing NaN/Inf
            if tensor.requires_grad:
                tensor = torch.where(
                    torch.isnan(tensor) | torch.isinf(tensor),
                    torch.zeros_like(tensor),
                    tensor
                )
            else:
                tensor = torch.nan_to_num(tensor, nan=0.0, posinf=10.0, neginf=-10.0)
        
        # Clamp to reasonable bounds while preserving gradients
        tensor = torch.clamp(tensor, min=-100.0, max=100.0)
        return tensor
        
    # ADD THESE NEW METHODS HERE:
    def _setup_enhanced_mixed_precision(self):
        """Enhanced mixed precision with dynamic loss scaling"""
        if not torch.cuda.is_available():
            return  # Skip if no CUDA
            
        self.scaler = torch.cuda.amp.GradScaler(
            init_scale=2**16,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=2000
        )
        
        # NOTE: Disabled automatic half precision conversion to avoid dtype mismatches
        # The autocast context will handle mixed precision automatically
        
        worker_logger.warning("Enhanced mixed precision training enabled", 
                            extra={'worker_id': get_ray_worker_id()})
    def _forward_train(self, batch: Dict[str, Any], **kwargs):
        """Training forward pass with AMP support and device management"""
        # Lightweight logging for learning activity
        self._learning_count += 1
        if self._learning_count % self._log_every_n == 0:
            worker_id = get_ray_worker_id()
            batch_size = batch.get('obs', {}).get('market', torch.tensor([])).shape[0] if 'obs' in batch else 0
            worker_logger.info(f"[LEARNING] Agent learned {self._learning_count} times (batch size: {batch_size})", 
                              extra={'worker_id': worker_id})
        
        # CRITICAL: Create attention masks from replay buffer padding before training
        batch, attention_mask = self._filter_padding_from_batch(batch)
        
        # CRITICAL: Ensure entire batch is on correct device
        batch = self._ensure_batch_device(batch)
        
        if hasattr(self, 'scaler'):
            with torch.cuda.amp.autocast():
                return self._forward_train_core(batch, attention_mask=attention_mask, **kwargs)
        else:
            return self._forward_train_core(batch, attention_mask=attention_mask, **kwargs)
    @override(TorchRLModule)
    def get_train_action_dist_cls(self):
        # Create hybrid distribution for discrete timing + continuous/discrete parameters
        timing_size = self.timing_action_size
        parameter_configs = self.parameter_configs
        
        class HybridActionDistribution:
            def __init__(self, inputs=None, input_lens=None, model=None):
                self.inputs = inputs
                self.timing_size = timing_size
                self.parameter_configs = parameter_configs
                
                if inputs is not None:
                    self._parse_inputs(inputs)
            
            def _parse_inputs(self, inputs):
                """Parse the concatenated inputs into timing and parameter components"""
                # Timing logits (discrete)
                self.timing_logits = inputs[..., :self.timing_size]
                
                # Parameter inputs (mixed continuous/discrete)
                self.parameter_inputs = []
                start_idx = self.timing_size
                
                for param_config in self.parameter_configs:
                    if param_config['type'] == 'continuous':
                        # Continuous parameter: mean and log_std
                        end_idx = start_idx + 2
                        param_input = inputs[..., start_idx:end_idx]
                        self.parameter_inputs.append(param_input)
                        start_idx = end_idx
                    else:
                        # Discrete parameter: logits
                        param_size = param_config['n']
                        end_idx = start_idx + param_size
                        param_input = inputs[..., start_idx:end_idx]
                        self.parameter_inputs.append(param_input)
                        start_idx = end_idx
            
            @classmethod
            def from_logits(cls, logits, input_lens=None, model=None):
                return cls(inputs=logits, input_lens=input_lens, model=model)
            
            def sample(self):
                """Sample from hybrid distribution WITHOUT clipping"""
                samples = []
                
                # Sample timing action (discrete)
                timing_dist = torch.distributions.Categorical(logits=self.timing_logits)
                timing_sample = timing_dist.sample()
                samples.append(timing_sample)
                
                # Sample parameter actions (mixed continuous/discrete)
                for i, param_config in enumerate(self.parameter_configs):
                    param_input = self.parameter_inputs[i]
                    
                    if param_config['type'] == 'continuous':
                        # Use the raw mean and log_std outputs directly
                        mean = param_input[..., 0]
                        log_std = param_input[..., 1]
                        std = torch.exp(log_std)  # No clipping on std either
                        
                        # Sample from unbounded normal
                        raw_sample = torch.distributions.Normal(mean, std).sample()
                        
                        # Transform to bounds using sigmoid (differentiable)
                        low, high = param_config['low'], param_config['high']
                        # Sigmoid maps (-inf, inf) to (0, 1), then scale to (low, high)
                        bounded_sample = low + torch.sigmoid(raw_sample) * (high - low)
                        
                        samples.append(bounded_sample)
                    else:
                        # Discrete parameter: sample from Categorical distribution
                        param_dist = torch.distributions.Categorical(logits=param_input)
                        param_sample = param_dist.sample().float()
                        samples.append(param_sample)
                
                return torch.stack(samples, dim=-1)
                        
            def logp(self, actions):
                """Compute log probability for hybrid actions"""
                log_probs = []
                
                # Timing action log probability (discrete)
                timing_dist = torch.distributions.Categorical(logits=self.timing_logits)
                timing_action = actions[..., 0].long()
                timing_logp = timing_dist.log_prob(timing_action)
                log_probs.append(timing_logp)
                
                # Parameter action log probabilities (mixed continuous/discrete)
                for i, param_config in enumerate(self.parameter_configs):
                    param_input = self.parameter_inputs[i]
                    param_action = actions[..., i + 1]  # +1 because timing is at index 0
                    
                    if param_config['type'] == 'continuous':
                        # Continuous parameter: Normal distribution log probability
                        mean = param_input[..., 0]
                        log_std = param_input[..., 1]
                        std = torch.exp(torch.clamp(log_std, min=-20, max=2))
                        
                        param_dist = torch.distributions.Normal(mean, std)
                        param_logp = param_dist.log_prob(param_action)
                    else:
                        # Discrete parameter: Categorical distribution log probability
                        param_dist = torch.distributions.Categorical(logits=param_input)
                        param_action_int = param_action.long()
                        param_logp = param_dist.log_prob(param_action_int)
                    
                    log_probs.append(param_logp)
                
                return torch.stack(log_probs, dim=-1).sum(dim=-1)
            
            def entropy(self):
                """Compute entropy for hybrid distribution"""
                entropies = []
                
                # Timing action entropy (discrete)
                timing_dist = torch.distributions.Categorical(logits=self.timing_logits)
                timing_entropy = timing_dist.entropy()
                entropies.append(timing_entropy)
                
                # Parameter action entropies (mixed continuous/discrete)
                for i, param_config in enumerate(self.parameter_configs):
                    param_input = self.parameter_inputs[i]
                    
                    if param_config['type'] == 'continuous':
                        # Continuous parameter: Normal distribution entropy
                        log_std = param_input[..., 1]
                        std = torch.exp(torch.clamp(log_std, min=-20, max=2))
                        
                        param_dist = torch.distributions.Normal(torch.zeros_like(std), std)
                        param_entropy = param_dist.entropy()
                    else:
                        # Discrete parameter: Categorical distribution entropy
                        param_dist = torch.distributions.Categorical(logits=param_input)
                        param_entropy = param_dist.entropy()
                    
                    entropies.append(param_entropy)
                
                return torch.stack(entropies, dim=-1).sum(dim=-1)
        
        return HybridActionDistribution
    
    @override(TorchRLModule)
    def get_exploration_action_dist_cls(self):
        return self.get_train_action_dist_cls()
    
    @override(TorchRLModule)
    def get_inference_action_dist_cls(self):
        return self.get_train_action_dist_cls()
